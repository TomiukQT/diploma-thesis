{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from re import subn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from analyzer import data_cleaner\n",
    "from analyzer.data_transformation import TfidfDataTransformer, BagOfWordsTransformer, DataTransformer\n",
    "\n",
    "from imblearn.under_sampling import TomekLinks,RandomUnderSampler, CondensedNearestNeighbour,EditedNearestNeighbours\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore', category=ConvergenceWarning)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "#Load data\n",
    "\n",
    "def load_and_clean_data(filepath, rename_dict=None, vectorizer_output='models/vectorizer.sav', **kwargs) -> (pd.DataFrame, DataTransformer):\n",
    "    # Load data\n",
    "    df = pd.read_csv(filepath, **kwargs)\n",
    "\n",
    "    if rename_dict is not None:\n",
    "        df.rename(columns=rename_dict, inplace=True)\n",
    "    #display(df.head(5))\n",
    "    # Clean data\n",
    "        # Remove @ mentions\n",
    "    df['clean_text'] = np.vectorize(data_cleaner.clean_mentions)(df['text'])\n",
    "        # Remove non alfabet chars\n",
    "    df['clean_text'] = df['clean_text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "        # Remove short words\n",
    "    df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "    # Transform\n",
    "    tr = TfidfDataTransformer()\n",
    "    # Stemming\n",
    "    df['clean_text'] = tr.stemming(df['clean_text'])\n",
    "\n",
    "    tr.vectorizer_fit(df['clean_text'])\n",
    "    df_tfidf = tr.transform(df['clean_text'])\n",
    "    pickle.dump(tr.vectorizer, open(vectorizer_output, 'wb'))\n",
    "\n",
    "\n",
    "    return df, df_tfidf, tr\n",
    "\n",
    "def balance_data(X, y, balancer = RandomUnderSampler(sampling_strategy='not minority',random_state=1337)) -> pd.DataFrame:\n",
    "    X_balanced, y_balanced = balancer.fit_resample(X, y)\n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "\n",
    "\n",
    "def fit_model(X, y, model_type, params, model_output='../models/model.sav', ):\n",
    "    model = model_type(**params)\n",
    "    model.fit(X,y)\n",
    "    if model_output is not None:\n",
    "        pickle.dump(model, open(model_output, 'wb'))\n",
    "    return model\n",
    "\n",
    "proba_models = set(['LogisticRegression', 'BernoulliNB', 'MLPClassifier', 'ComplementNB'])\n",
    "def predict(model, data):\n",
    "    model_name = type(model).__name__\n",
    "    if model_name in proba_models:\n",
    "        predictions = model.predict_proba(data)\n",
    "    else:\n",
    "        predictions = model.predict(data)\n",
    "    return predictions\n",
    "\n",
    "def metrics(predictions, true_values, name_prefix=\"\", plot=True):\n",
    "    predictions_int =  predictions[:,1]>=0.3\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'f1_score': f1_score(true_values, predictions_int),\n",
    "        'confusion_matrix': confusion_matrix(true_values, predictions_int)\n",
    "    }\n",
    "    # Plot?? mby\n",
    "    if plot:\n",
    "        ConfusionMatrixDisplay.from_predictions(true_values, predictions_int)\n",
    "        plt.show()\n",
    "    # Write to outputfile.\n",
    "    with open(f'out/results/{name_prefix}_out.out', 'w') as f:\n",
    "            f.write(str(metrics))\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def clean_folders():\n",
    "    for folder, end in [('models', '.sav'), ('out/results', '.out')]:\n",
    "        filelist = [ f for f in os.listdir(folder) if f.endswith(end) ]\n",
    "        for f in filelist:\n",
    "            os.remove(os.path.join(folder, f))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def __model_tuning__(X_train, y_train, X_valid, y_valid, model_type,params, output_file, data_label=\"\", verbose=False, plot=False):\n",
    "    output_name = f'{model_type.__name__}-$-{data_label}-{datetime.now().timestamp()}'\n",
    "    model_output = f'models/{output_name}.sav'\n",
    "    outputs = []\n",
    "    if verbose:\n",
    "        print(f'Starting tuning of {model_type.__name__} with data labeled: {data_label}')\n",
    "        print('===============')\n",
    "    parameter_grid = ParameterGrid(params)\n",
    "    for i, p in enumerate(parameter_grid):\n",
    "        param_string = subn(\"[{}',:]\",\"\",\"\".join(str(p).split()))[0]\n",
    "        if verbose:\n",
    "            print(f'Params {i+1}/{len(parameter_grid)}: {p}')\n",
    "        model_output = model_output.replace('$',param_string)\n",
    "        model = fit_model(X_train, y_train, model_type, params=p, model_output=model_output)\n",
    "        predictions = predict(model, X_valid)\n",
    "        output = metrics(predictions, y_valid, output_name.replace('$',param_string),plot=plot)\n",
    "        outputs.append(output)\n",
    "        if verbose:\n",
    "            print(f'Output: {output}')\n",
    "    if verbose:\n",
    "        print(f'Tuning ended')\n",
    "        print('===============')\n",
    "    return outputs\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "clean_folders()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "==ComplementNB==\n",
      "=====================\n",
      "Starting tuning of ComplementNB with data labeled: bal\n",
      "===============\n",
      "Params 1/20: {'alpha': 0.0, 'fit_prior': True, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 2/20: {'alpha': 0.0, 'fit_prior': True, 'norm': False}\n",
      "Output: {'f1_score': 0.3070767691922981, 'confusion_matrix': array([[6204, 2705],\n",
      "       [  66,  614]], dtype=int64)}\n",
      "Params 3/20: {'alpha': 0.0, 'fit_prior': False, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 4/20: {'alpha': 0.0, 'fit_prior': False, 'norm': False}\n",
      "Output: {'f1_score': 0.3070767691922981, 'confusion_matrix': array([[6204, 2705],\n",
      "       [  66,  614]], dtype=int64)}\n",
      "Params 5/20: {'alpha': 0.2, 'fit_prior': True, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 6/20: {'alpha': 0.2, 'fit_prior': True, 'norm': False}\n",
      "Output: {'f1_score': 0.292099322799097, 'confusion_matrix': array([[5806, 3103],\n",
      "       [  33,  647]], dtype=int64)}\n",
      "Params 7/20: {'alpha': 0.2, 'fit_prior': False, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 8/20: {'alpha': 0.2, 'fit_prior': False, 'norm': False}\n",
      "Output: {'f1_score': 0.292099322799097, 'confusion_matrix': array([[5806, 3103],\n",
      "       [  33,  647]], dtype=int64)}\n",
      "Params 9/20: {'alpha': 0.4, 'fit_prior': True, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 10/20: {'alpha': 0.4, 'fit_prior': True, 'norm': False}\n",
      "Output: {'f1_score': 0.2800940774000427, 'confusion_matrix': array([[5567, 3342],\n",
      "       [  25,  655]], dtype=int64)}\n",
      "Params 11/20: {'alpha': 0.4, 'fit_prior': False, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 12/20: {'alpha': 0.4, 'fit_prior': False, 'norm': False}\n",
      "Output: {'f1_score': 0.2800940774000427, 'confusion_matrix': array([[5567, 3342],\n",
      "       [  25,  655]], dtype=int64)}\n",
      "Params 13/20: {'alpha': 0.6000000000000001, 'fit_prior': True, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 14/20: {'alpha': 0.6000000000000001, 'fit_prior': True, 'norm': False}\n",
      "Output: {'f1_score': 0.2677261613691932, 'confusion_matrix': array([[5338, 3571],\n",
      "       [  23,  657]], dtype=int64)}\n",
      "Params 15/20: {'alpha': 0.6000000000000001, 'fit_prior': False, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 16/20: {'alpha': 0.6000000000000001, 'fit_prior': False, 'norm': False}\n",
      "Output: {'f1_score': 0.2677261613691932, 'confusion_matrix': array([[5338, 3571],\n",
      "       [  23,  657]], dtype=int64)}\n",
      "Params 17/20: {'alpha': 0.8, 'fit_prior': True, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 18/20: {'alpha': 0.8, 'fit_prior': True, 'norm': False}\n",
      "Output: {'f1_score': 0.25752246971473236, 'confusion_matrix': array([[5130, 3779],\n",
      "       [  21,  659]], dtype=int64)}\n",
      "Params 19/20: {'alpha': 0.8, 'fit_prior': False, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 20/20: {'alpha': 0.8, 'fit_prior': False, 'norm': False}\n",
      "Output: {'f1_score': 0.25752246971473236, 'confusion_matrix': array([[5130, 3779],\n",
      "       [  21,  659]], dtype=int64)}\n",
      "Tuning ended\n",
      "===============\n",
      "Starting tuning of ComplementNB with data labeled: unba\n",
      "===============\n",
      "Params 1/20: {'alpha': 0.0, 'fit_prior': True, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 2/20: {'alpha': 0.0, 'fit_prior': True, 'norm': False}\n",
      "Output: {'f1_score': 0.324150842471249, 'confusion_matrix': array([[6456, 2453],\n",
      "       [  74,  606]], dtype=int64)}\n",
      "Params 3/20: {'alpha': 0.0, 'fit_prior': False, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 4/20: {'alpha': 0.0, 'fit_prior': False, 'norm': False}\n",
      "Output: {'f1_score': 0.324150842471249, 'confusion_matrix': array([[6456, 2453],\n",
      "       [  74,  606]], dtype=int64)}\n",
      "Params 5/20: {'alpha': 0.2, 'fit_prior': True, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 6/20: {'alpha': 0.2, 'fit_prior': True, 'norm': False}\n",
      "Output: {'f1_score': 0.3045949786830886, 'confusion_matrix': array([[6010, 2899],\n",
      "       [  37,  643]], dtype=int64)}\n",
      "Params 7/20: {'alpha': 0.2, 'fit_prior': False, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 8/20: {'alpha': 0.2, 'fit_prior': False, 'norm': False}\n",
      "Output: {'f1_score': 0.3045949786830886, 'confusion_matrix': array([[6010, 2899],\n",
      "       [  37,  643]], dtype=int64)}\n",
      "Params 9/20: {'alpha': 0.4, 'fit_prior': True, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 10/20: {'alpha': 0.4, 'fit_prior': True, 'norm': False}\n",
      "Output: {'f1_score': 0.28908160996219706, 'confusion_matrix': array([[5742, 3167],\n",
      "       [  30,  650]], dtype=int64)}\n",
      "Params 11/20: {'alpha': 0.4, 'fit_prior': False, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 12/20: {'alpha': 0.4, 'fit_prior': False, 'norm': False}\n",
      "Output: {'f1_score': 0.28908160996219706, 'confusion_matrix': array([[5742, 3167],\n",
      "       [  30,  650]], dtype=int64)}\n",
      "Params 13/20: {'alpha': 0.6000000000000001, 'fit_prior': True, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 14/20: {'alpha': 0.6000000000000001, 'fit_prior': True, 'norm': False}\n",
      "Output: {'f1_score': 0.2763713080168776, 'confusion_matrix': array([[5504, 3405],\n",
      "       [  25,  655]], dtype=int64)}\n",
      "Params 15/20: {'alpha': 0.6000000000000001, 'fit_prior': False, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 16/20: {'alpha': 0.6000000000000001, 'fit_prior': False, 'norm': False}\n",
      "Output: {'f1_score': 0.2763713080168776, 'confusion_matrix': array([[5504, 3405],\n",
      "       [  25,  655]], dtype=int64)}\n",
      "Params 17/20: {'alpha': 0.8, 'fit_prior': True, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 18/20: {'alpha': 0.8, 'fit_prior': True, 'norm': False}\n",
      "Output: {'f1_score': 0.26674786845310594, 'confusion_matrix': array([[5320, 3589],\n",
      "       [  23,  657]], dtype=int64)}\n",
      "Params 19/20: {'alpha': 0.8, 'fit_prior': False, 'norm': True}\n",
      "Output: {'f1_score': 0.13243743305092998, 'confusion_matrix': array([[   0, 8909],\n",
      "       [   0,  680]], dtype=int64)}\n",
      "Params 20/20: {'alpha': 0.8, 'fit_prior': False, 'norm': False}\n",
      "Output: {'f1_score': 0.26674786845310594, 'confusion_matrix': array([[5320, 3589],\n",
      "       [  23,  657]], dtype=int64)}\n",
      "Tuning ended\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    (LogisticRegression, {\n",
    "        'penalty': ['none', 'l2'],\n",
    "        'class_weight': [None,'balanced'],\n",
    "        'n_jobs': [-1],\n",
    "        'solver': ['newton-cholesky']\n",
    "    }),\n",
    "    (BernoulliNB, {\n",
    "        'alpha' : np.arange(0.0, 1.0, 0.2),\n",
    "        'fit_prior': [True, False],\n",
    "        'binarize': [None]\n",
    "\n",
    "    }),\n",
    "    (MLPClassifier, {\n",
    "        'hidden_layer_sizes': [(100,), (100, 100), (50, 50, 50), (50,)],\n",
    "        'activation': ['identity', 'logistic'],\n",
    "        'alpha': [0.0001, 0.001]\n",
    "    }),\n",
    "    (ComplementNB,{\n",
    "        'alpha' : np.arange(0.0, 1.0, 0.2),\n",
    "        'fit_prior': [True, False],\n",
    "        'norm': [True, False],\n",
    "    })\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "verbose = True\n",
    "random_state = 1337\n",
    "for model, param_grid in models[-1:]:\n",
    "    if verbose:\n",
    "        print('=====================')\n",
    "        print(f'=={model.__name__}==')\n",
    "        print('=====================')\n",
    "\n",
    "    df, df_tfidf, tr = load_and_clean_data('data/train.csv', rename_dict={'tweet':'text', 'label':'output'})\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(df_tfidf, df['output'],test_size=0.3,random_state=random_state)\n",
    "    X_train_balanced, y_train_balanced = balance_data(X_train, y_train)\n",
    "\n",
    "    output = __model_tuning__(X_train_balanced, y_train_balanced, X_valid, y_valid, model, param_grid, 'lin_reg_1' ,'bal', verbose=verbose)\n",
    "    output2 = __model_tuning__(X_train, y_train, X_valid, y_valid, model, param_grid, 'lin_reg_1' ,'unba', verbose=verbose)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
