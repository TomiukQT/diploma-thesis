{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from re import subn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from analyzer import data_cleaner\n",
    "from analyzer.data_transformation import TfidfDataTransformer, BagOfWordsTransformer, DataTransformer\n",
    "\n",
    "from imblearn.under_sampling import TomekLinks,RandomUnderSampler, CondensedNearestNeighbour,EditedNearestNeighbours\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore', category=ConvergenceWarning)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "#Load data\n",
    "\n",
    "def load_and_clean_data(filepath, rename_dict=None, vectorizer_output='models/vectorizer.sav', **kwargs) -> (pd.DataFrame, DataTransformer):\n",
    "    # Load data\n",
    "    df = pd.read_csv(filepath, **kwargs)\n",
    "\n",
    "    if rename_dict is not None:\n",
    "        df.rename(columns=rename_dict, inplace=True)\n",
    "    #display(df.head(5))\n",
    "    # Clean data\n",
    "        # Remove @ mentions\n",
    "    df['clean_text'] = np.vectorize(data_cleaner.clean_mentions)(df['text'])\n",
    "        # Remove non alfabet chars\n",
    "    df['clean_text'] = df['clean_text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "        # Remove short words\n",
    "    df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "    # Transform\n",
    "    tr = TfidfDataTransformer()\n",
    "    # Stemming\n",
    "    df['clean_text'] = tr.stemming(df['clean_text'])\n",
    "\n",
    "    tr.vectorizer_fit(df['clean_text'])\n",
    "    df_tfidf = tr.transform(df['clean_text'])\n",
    "    pickle.dump(tr.vectorizer, open(vectorizer_output, 'wb'))\n",
    "\n",
    "\n",
    "    return df, df_tfidf, tr\n",
    "\n",
    "def balance_data(X, y, balancer = RandomUnderSampler(sampling_strategy='not minority',random_state=1337)) -> pd.DataFrame:\n",
    "    X_balanced, y_balanced = balancer.fit_resample(X, y)\n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "\n",
    "\n",
    "def fit_model(X, y, model_type, params, model_output='../models/model.sav', ):\n",
    "    model = model_type(**params)\n",
    "    model.fit(X,y)\n",
    "    if model_output is not None:\n",
    "        pickle.dump(model, open(model_output, 'wb'))\n",
    "    return model\n",
    "\n",
    "proba_models = set(['LogisticRegression', 'BernoulliNB'])\n",
    "def predict(model, data):\n",
    "    model_name = type(model).__name__\n",
    "    if model_name in proba_models:\n",
    "        predictions = model.predict_proba(data)\n",
    "    else:\n",
    "        predictions = model.predict(data)\n",
    "    return predictions\n",
    "\n",
    "def metrics(predictions, true_values, name_prefix=\"\", plot=True):\n",
    "    predictions_int =  predictions[:,1]>=0.3\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'f1_score': f1_score(true_values, predictions_int),\n",
    "        'confusion_matrix': confusion_matrix(true_values, predictions_int)\n",
    "    }\n",
    "    # Plot?? mby\n",
    "    if plot:\n",
    "        ConfusionMatrixDisplay.from_predictions(true_values, predictions_int)\n",
    "        plt.show()\n",
    "    # Write to outputfile.\n",
    "    with open(f'out/results/{name_prefix}_out.out', 'w') as f:\n",
    "            f.write(str(metrics))\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def clean_folders():\n",
    "    for folder, end in [('models', '.sav'), ('out/results', '.out')]:\n",
    "        filelist = [ f for f in os.listdir(folder) if f.endswith(end) ]\n",
    "        for f in filelist:\n",
    "            os.remove(os.path.join(folder, f))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def __model_tuning__(X_train, y_train, X_valid, y_valid, model_type,params, output_file, data_label=\"\", verbose=False, plot=False):\n",
    "    output_name = f'{model_type.__name__}-$-{data_label}-{datetime.now().timestamp()}'\n",
    "    model_output = f'models/{output_name}.sav'\n",
    "    outputs = []\n",
    "    if verbose:\n",
    "        print(f'Starting tuning of {model_type.__name__} with data labeled: {data_label}')\n",
    "        print('===============')\n",
    "    parameter_grid = ParameterGrid(params)\n",
    "    for i, p in enumerate(parameter_grid):\n",
    "        param_string = subn(\"[{}',:]\",\"\",\"\".join(str(p).split()))[0]\n",
    "        if verbose:\n",
    "            print(f'Params {i+1}/{len(parameter_grid)}: {p}')\n",
    "        model_output = model_output.replace('$',param_string)\n",
    "        model = fit_model(X_train, y_train, model_type, params=p, model_output=model_output)\n",
    "        predictions = predict(model, X_valid)\n",
    "        output = metrics(predictions, y_valid, output_name.replace('$',param_string),plot=plot)\n",
    "        outputs.append(output)\n",
    "        if verbose:\n",
    "            print(f'Output: {output}')\n",
    "    if verbose:\n",
    "        print(f'Tuning ended')\n",
    "        print('===============')\n",
    "    return outputs\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "clean_folders()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "==LogisticRegression==\n",
      "=====================\n",
      "Starting tuning of LogisticRegression with data labeled: bal\n",
      "===============\n",
      "Params 1/4: {'class_weight': None, 'max_iter': 400, 'n_jobs': -1, 'penalty': 'none'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomas.valenta/School/diploma_thesis/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {'f1_score': 0.31820895522388065, 'confusion_matrix': array([[6772, 2137],\n",
      "       [ 147,  533]])}\n",
      "Params 2/4: {'class_weight': None, 'max_iter': 400, 'n_jobs': -1, 'penalty': 'l2'}\n",
      "Output: {'f1_score': 0.25910612325260873, 'confusion_matrix': array([[5168, 3741],\n",
      "       [  22,  658]])}\n",
      "Params 3/4: {'class_weight': 'balanced', 'max_iter': 400, 'n_jobs': -1, 'penalty': 'none'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomas.valenta/School/diploma_thesis/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {'f1_score': 0.31820895522388065, 'confusion_matrix': array([[6772, 2137],\n",
      "       [ 147,  533]])}\n",
      "Params 4/4: {'class_weight': 'balanced', 'max_iter': 400, 'n_jobs': -1, 'penalty': 'l2'}\n",
      "Output: {'f1_score': 0.25910612325260873, 'confusion_matrix': array([[5168, 3741],\n",
      "       [  22,  658]])}\n",
      "Tuning ended\n",
      "===============\n",
      "Starting tuning of LogisticRegression with data labeled: unba\n",
      "===============\n",
      "Params 1/4: {'class_weight': None, 'max_iter': 400, 'n_jobs': -1, 'penalty': 'none'}\n",
      "Output: {'f1_score': 0.552821997105644, 'confusion_matrix': array([[8589,  320],\n",
      "       [ 298,  382]])}\n",
      "Params 2/4: {'class_weight': None, 'max_iter': 400, 'n_jobs': -1, 'penalty': 'l2'}\n",
      "Output: {'f1_score': 0.5508771929824561, 'confusion_matrix': array([[8763,  146],\n",
      "       [ 366,  314]])}\n",
      "Params 3/4: {'class_weight': 'balanced', 'max_iter': 400, 'n_jobs': -1, 'penalty': 'none'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomas.valenta/School/diploma_thesis/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {'f1_score': 0.4015918958031837, 'confusion_matrix': array([[7380, 1529],\n",
      "       [ 125,  555]])}\n",
      "Params 4/4: {'class_weight': 'balanced', 'max_iter': 400, 'n_jobs': -1, 'penalty': 'l2'}\n",
      "Output: {'f1_score': 0.35243798118049613, 'confusion_matrix': array([[6700, 2209],\n",
      "       [  62,  618]])}\n",
      "Tuning ended\n",
      "===============\n",
      "=====================\n",
      "==BernoulliNB==\n",
      "=====================\n",
      "Starting tuning of BernoulliNB with data labeled: bal\n",
      "===============\n",
      "Params 1/6: {'alpha': 0, 'binarize': None, 'fit_prior': True}\n",
      "Output: {'f1_score': 0.3077694235588973, 'confusion_matrix': array([[6213, 2696],\n",
      "       [  66,  614]])}\n",
      "Params 2/6: {'alpha': 0, 'binarize': None, 'fit_prior': False}\n",
      "Output: {'f1_score': 0.3077694235588973, 'confusion_matrix': array([[6213, 2696],\n",
      "       [  66,  614]])}\n",
      "Params 3/6: {'alpha': 0.5, 'binarize': None, 'fit_prior': True}\n",
      "Output: {'f1_score': 0.2743787847149718, 'confusion_matrix': array([[5457, 3452],\n",
      "       [  23,  657]])}\n",
      "Params 4/6: {'alpha': 0.5, 'binarize': None, 'fit_prior': False}\n",
      "Output: {'f1_score': 0.2743787847149718, 'confusion_matrix': array([[5457, 3452],\n",
      "       [  23,  657]])}\n",
      "Params 5/6: {'alpha': 1, 'binarize': None, 'fit_prior': True}\n",
      "Output: {'f1_score': 0.24976419543482362, 'confusion_matrix': array([[4950, 3959],\n",
      "       [  18,  662]])}\n",
      "Params 6/6: {'alpha': 1, 'binarize': None, 'fit_prior': False}\n",
      "Output: {'f1_score': 0.24976419543482362, 'confusion_matrix': array([[4950, 3959],\n",
      "       [  18,  662]])}\n",
      "Tuning ended\n",
      "===============\n",
      "Starting tuning of BernoulliNB with data labeled: unba\n",
      "===============\n",
      "Params 1/6: {'alpha': 0, 'binarize': None, 'fit_prior': True}\n",
      "Output: {'f1_score': 0.5154061624649859, 'confusion_matrix': array([[8794,  115],\n",
      "       [ 404,  276]])}\n",
      "Params 2/6: {'alpha': 0, 'binarize': None, 'fit_prior': False}\n",
      "Output: {'f1_score': 0.32467184570050894, 'confusion_matrix': array([[6462, 2447],\n",
      "       [  74,  606]])}\n",
      "Params 3/6: {'alpha': 0.5, 'binarize': None, 'fit_prior': True}\n",
      "Output: {'f1_score': 0.4985835694050992, 'confusion_matrix': array([[8794,  115],\n",
      "       [ 416,  264]])}\n",
      "Params 4/6: {'alpha': 0.5, 'binarize': None, 'fit_prior': False}\n",
      "Output: {'f1_score': 0.2842174197773412, 'confusion_matrix': array([[5659, 3250],\n",
      "       [  29,  651]])}\n",
      "Params 5/6: {'alpha': 1, 'binarize': None, 'fit_prior': True}\n",
      "Output: {'f1_score': 0.48222862632084534, 'confusion_matrix': array([[8799,  110],\n",
      "       [ 429,  251]])}\n",
      "Params 6/6: {'alpha': 1, 'binarize': None, 'fit_prior': False}\n",
      "Output: {'f1_score': 0.2672962430712379, 'confusion_matrix': array([[5369, 3540],\n",
      "       [  29,  651]])}\n",
      "Tuning ended\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    (LogisticRegression, {\n",
    "        'penalty': ['none', 'l2'],\n",
    "        'class_weight': [None,'balanced'],\n",
    "        'n_jobs': [-1],\n",
    "        'max_iter': [400]\n",
    "    }),\n",
    "    (BernoulliNB, {\n",
    "        'alpha': [0, 0.5, 1],\n",
    "        'fit_prior': [True, False],\n",
    "        'binarize': [None]\n",
    "\n",
    "    })\n",
    "\n",
    "]\n",
    "\n",
    "verbose = True\n",
    "random_state = 1337\n",
    "for model, param_grid in models:\n",
    "    if verbose:\n",
    "        print('=====================')\n",
    "        print(f'=={model.__name__}==')\n",
    "        print('=====================')\n",
    "\n",
    "    df, df_tfidf, tr = load_and_clean_data('data/train.csv', rename_dict={'tweet':'text', 'label':'output'})\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(df_tfidf, df['output'],test_size=0.3,random_state=random_state)\n",
    "    X_train_balanced, y_train_balanced = balance_data(X_train, y_train)\n",
    "\n",
    "    output = __model_tuning__(X_train_balanced, y_train_balanced, X_valid, y_valid, model, param_grid, 'lin_reg_1' ,'bal', verbose=verbose)\n",
    "    output2 = __model_tuning__(X_train, y_train, X_valid, y_valid, model, param_grid, 'lin_reg_1' ,'unba', verbose=verbose)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
