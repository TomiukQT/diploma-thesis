{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "from re import subn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, average_precision_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from analyzer import data_cleaner\n",
    "from analyzer.data_transformation import TfidfDataTransformer, BagOfWordsTransformer, DataTransformer\n",
    "\n",
    "from imblearn.under_sampling import TomekLinks, RandomUnderSampler, CondensedNearestNeighbour,EditedNearestNeighbours\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Run this cell if you want to ignore warnings\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from scipy.linalg import LinAlgWarning\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore', category=LinAlgWarning, module='sklearn')\n",
    "warnings.simplefilter('ignore', category=ConvergenceWarning)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def load_and_clean_data(filepath, rename_dict=None, vectorizer_output='models/vectorizer.sav', tr=None, **kwargs) -> (pd.DataFrame, DataTransformer):\n",
    "    # Load data\n",
    "    df = pd.read_csv(filepath, **kwargs)\n",
    "\n",
    "    if rename_dict is not None:\n",
    "        df.rename(columns=rename_dict, inplace=True)\n",
    "    #display(df.head(5))\n",
    "    # Clean data\n",
    "        # Remove @ mentions\n",
    "    df['clean_text'] = np.vectorize(data_cleaner.clean_mentions)(df['text'])\n",
    "        # Remove non alfabet chars\n",
    "    df['clean_text'] = df['clean_text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "        # Remove short words\n",
    "    df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "    # Transform\n",
    "    #tr = TfidfDataTransformer()\n",
    "    if tr is None:\n",
    "        tr = BagOfWordsTransformer()\n",
    "    # Stemming\n",
    "    df['clean_text'] = tr.stemming(df['clean_text'])\n",
    "\n",
    "    tr.vectorizer_fit(df['clean_text'])\n",
    "    df_tfidf = tr.transform(df['clean_text'])\n",
    "    pickle.dump(tr.vectorizer, open(vectorizer_output, 'wb'))\n",
    "\n",
    "\n",
    "    return df, df_tfidf, tr\n",
    "\n",
    "def balance_data(X, y, balancer = RandomUnderSampler(sampling_strategy='not minority',random_state=1337)) -> pd.DataFrame:\n",
    "    X_balanced, y_balanced = balancer.fit_resample(X, y)\n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "\n",
    "\n",
    "def fit_model(X, y, model_type, params, model_output='../models/model.sav', ):\n",
    "    model = model_type(**params)\n",
    "    model.fit(X,y)\n",
    "    if model_output is not None:\n",
    "        pickle.dump(model, open(model_output, 'wb'))\n",
    "    return model\n",
    "\n",
    "proba_models = set(['LogisticRegression', 'BernoulliNB', 'MLPClassifier', 'ComplementNB'])\n",
    "def predict(model, data):\n",
    "    model_name = type(model).__name__\n",
    "    model_type = 'probabilistic' if model_name in proba_models else 'other'\n",
    "    if model_name in proba_models:\n",
    "        predictions = model.predict_proba(data)\n",
    "    else:\n",
    "        predictions = model.predict(data)\n",
    "    return predictions, model_type\n",
    "\n",
    "def metrics(predictions, true_values, name_prefix=\"\", plot=True, model_class='probabilistic', output_df=None):\n",
    "\n",
    "    if model_class == 'probabilistic':\n",
    "        predictions_int =  predictions[:,1]>=0.5\n",
    "        predictions = predictions[:,1]\n",
    "    else:\n",
    "        predictions_int = predictions >= 0.5\n",
    "    # Calculate metrics\n",
    "    tn, fp, fn, tp = confusion_matrix(true_values, predictions_int).ravel()\n",
    "    metrics = {\n",
    "        'f1_score': f1_score(true_values, predictions_int),\n",
    "        #'confusion_matrix': confusion_matrix(true_values, predictions_int),\n",
    "        'tn': tn,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'tp': tp,\n",
    "        'roc_auc_score': roc_auc_score(true_values, predictions),\n",
    "        'average_precision_score': average_precision_score(true_values, predictions),\n",
    "        'accuracy_score': accuracy_score(true_values, predictions_int),\n",
    "        'name': name_prefix\n",
    "    }\n",
    "    # Plot?? mby\n",
    "    if plot:\n",
    "        ConfusionMatrixDisplay.from_predictions(true_values, predictions_int)\n",
    "        plt.show()\n",
    "    # Write to outputfile.\n",
    "    if output_df is None:\n",
    "        output_df = pd.DataFrame.from_records(metrics, index=[0])\n",
    "    else:\n",
    "        output_df = output_df.append(pd.DataFrame.from_dict(metrics, orient='index').T)\n",
    "    with open(f'out/results/{name_prefix}_out.out', 'w') as f:\n",
    "        f.write(str(metrics))\n",
    "    return metrics, output_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def clean_folders():\n",
    "    for folder, end in [('models', '.sav'), ('out/results', '.out')]:\n",
    "        filelist = [ f for f in os.listdir(folder) if f.endswith(end) ]\n",
    "        for f in filelist:\n",
    "            os.remove(os.path.join(folder, f))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def analyze_sentence(text, model_file, vectorizer_file):\n",
    "    text = pd.Series([text])\n",
    "    #text = np.vectorize(data_cleaner.clean_mentions)(text)\n",
    "    # Remove non alfabet chars\n",
    "    text = text.str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    # Remove short words\n",
    "    text = text.apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "    # Transform\n",
    "    tr = TfidfDataTransformer(vectorizer_file)\n",
    "    # Stemming\n",
    "    text = tr.stemming(text)\n",
    "    df_tfidf = tr.transform(text)\n",
    "    model = pickle.load(open(f'{model_file}', 'rb'))\n",
    "    return model.predict_proba(df_tfidf)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def __model_tuning__(X_train, y_train, X_valid, y_valid, model_type,params, output_df, data_label=\"\", verbose=False, plot=False):\n",
    "    output_name = f'{model_type.__name__}-$-{data_label}'\n",
    "    _model_output = f'models/{output_name}.sav'\n",
    "    outputs = []\n",
    "    if verbose:\n",
    "        print(f'Starting tuning of {model_type.__name__} with data labeled: {data_label}')\n",
    "        print('===============')\n",
    "    parameter_grid = ParameterGrid(params)\n",
    "    for i, p in enumerate(parameter_grid):\n",
    "        param_string = subn(\"[{}',:]\",\"\",\"\".join(str(p).split()))[0]\n",
    "        if verbose:\n",
    "            print(f'Params {i+1}/{len(parameter_grid)}: {p}')\n",
    "        model_output = _model_output.replace('$',param_string)\n",
    "        model = fit_model(X_train, y_train, model_type, params=p, model_output=model_output)\n",
    "        predictions, model_class = predict(model, X_valid)\n",
    "        print(predictions)\n",
    "        output, output_df = metrics(predictions, y_valid, output_name.replace('$',param_string),plot=plot, model_class=model_class, output_df=output_df)\n",
    "        outputs.append(output)\n",
    "        print(f'Outputdf: {output_df.shape}')\n",
    "        if verbose:\n",
    "            print(f'Output: {output}')\n",
    "    if verbose:\n",
    "        print(f'Tuning ended')\n",
    "        print('===============')\n",
    "    return outputs, output_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "clean_folders()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "verbose = True\n",
    "random_state = 1337"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "==TfidfDataTransformer==\n",
      "=====================\n",
      "=====================\n",
      "==KNeighborsRegressor==\n",
      "=====================\n",
      "Starting tuning of KNeighborsRegressor with data labeled: -tfidf-ngram\n",
      "===============\n",
      "Params 1/42: {'leaf_size': 40, 'n_jobs': -1, 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "[0.         0.         0.         ... 0.33333333 0.         0.33333333]\n",
      "Outputdf: (1, 9)\n",
      "Output: {'f1_score': 0.38449367088607594, 'tn': 8568, 'fp': 341, 'fn': 437, 'tp': 243, 'roc_auc_score': 0.731059470594838, 'average_precision_score': 0.2834161936563818, 'accuracy_score': 0.9188653665658567, 'name': 'KNeighborsRegressor-leaf_size40n_jobs-1n_neighbors3weightsuniform--tfidf-ngram'}\n",
      "Params 2/42: {'leaf_size': 40, 'n_jobs': -1, 'n_neighbors': 3, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.32418799 0.         0.3195202 ]\n",
      "Outputdf: (2, 9)\n",
      "Output: {'f1_score': 0.4383975812547241, 'tn': 8556, 'fp': 353, 'fn': 390, 'tp': 290, 'roc_auc_score': 0.7355196001399774, 'average_precision_score': 0.34326111256533004, 'accuracy_score': 0.9225153822087809, 'name': 'KNeighborsRegressor-leaf_size40n_jobs-1n_neighbors3weightsdistance--tfidf-ngram'}\n",
      "Params 3/42: {'leaf_size': 40, 'n_jobs': -1, 'n_neighbors': 5, 'weights': 'uniform'}\n",
      "[0.  0.  0.  ... 0.2 0.  0.4]\n",
      "Outputdf: (3, 9)\n",
      "Output: {'f1_score': 0.35282837967401726, 'tn': 8730, 'fp': 179, 'fn': 496, 'tp': 184, 'roc_auc_score': 0.7299763788105882, 'average_precision_score': 0.3334241225627842, 'accuracy_score': 0.9296068411721764, 'name': 'KNeighborsRegressor-leaf_size40n_jobs-1n_neighbors5weightsuniform--tfidf-ngram'}\n",
      "Params 4/42: {'leaf_size': 40, 'n_jobs': -1, 'n_neighbors': 5, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.19706095 0.         0.38549401]\n",
      "Outputdf: (4, 9)\n",
      "Output: {'f1_score': 0.4338433843384339, 'tn': 8719, 'fp': 190, 'fn': 439, 'tp': 241, 'roc_auc_score': 0.735582986140915, 'average_precision_score': 0.40001522977368864, 'accuracy_score': 0.9344040045885911, 'name': 'KNeighborsRegressor-leaf_size40n_jobs-1n_neighbors5weightsdistance--tfidf-ngram'}\n",
      "Params 5/42: {'leaf_size': 40, 'n_jobs': -1, 'n_neighbors': 10, 'weights': 'uniform'}\n",
      "[0.  0.  0.  ... 0.1 0.  0.4]\n",
      "Outputdf: (5, 9)\n",
      "Output: {'f1_score': 0.31058823529411766, 'tn': 8871, 'fp': 38, 'fn': 548, 'tp': 132, 'roc_auc_score': 0.729430004687923, 'average_precision_score': 0.3415865995469962, 'accuracy_score': 0.9388883095213265, 'name': 'KNeighborsRegressor-leaf_size40n_jobs-1n_neighbors10weightsuniform--tfidf-ngram'}\n",
      "Params 6/42: {'leaf_size': 40, 'n_jobs': -1, 'n_neighbors': 10, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.09950816 0.         0.39584814]\n",
      "Outputdf: (6, 9)\n",
      "Output: {'f1_score': 0.45294725956566706, 'tn': 8841, 'fp': 68, 'fn': 461, 'tp': 219, 'roc_auc_score': 0.7349175981987811, 'average_precision_score': 0.40423774369595533, 'accuracy_score': 0.9448326207112316, 'name': 'KNeighborsRegressor-leaf_size40n_jobs-1n_neighbors10weightsdistance--tfidf-ngram'}\n",
      "Params 7/42: {'leaf_size': 40, 'n_jobs': -1, 'n_neighbors': 15, 'weights': 'uniform'}\n",
      "[0.         0.         0.         ... 0.06666667 0.         0.26666667]\n",
      "Outputdf: (7, 9)\n",
      "Output: {'f1_score': 0.2518891687657431, 'tn': 8895, 'fp': 14, 'fn': 580, 'tp': 100, 'roc_auc_score': 0.7293014994750847, 'average_precision_score': 0.3393128731235294, 'accuracy_score': 0.9380540202315153, 'name': 'KNeighborsRegressor-leaf_size40n_jobs-1n_neighbors15weightsuniform--tfidf-ngram'}\n",
      "Params 8/42: {'leaf_size': 40, 'n_jobs': -1, 'n_neighbors': 15, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.06655892 0.         0.27231511]\n",
      "Outputdf: (8, 9)\n",
      "Output: {'f1_score': 0.422077922077922, 'tn': 8860, 'fp': 49, 'fn': 485, 'tp': 195, 'roc_auc_score': 0.7345962938997578, 'average_precision_score': 0.3983552913286259, 'accuracy_score': 0.9443111899050995, 'name': 'KNeighborsRegressor-leaf_size40n_jobs-1n_neighbors15weightsdistance--tfidf-ngram'}\n",
      "Params 9/42: {'leaf_size': 40, 'n_jobs': -1, 'n_neighbors': 20, 'weights': 'uniform'}\n",
      "[0.   0.   0.   ... 0.05 0.   0.2 ]\n",
      "Outputdf: (9, 9)\n",
      "Output: {'f1_score': 0.21875, 'tn': 8905, 'fp': 4, 'fn': 596, 'tp': 84, 'roc_auc_score': 0.7292641116385943, 'average_precision_score': 0.339053709719031, 'accuracy_score': 0.9374283032641568, 'name': 'KNeighborsRegressor-leaf_size40n_jobs-1n_neighbors20weightsuniform--tfidf-ngram'}\n",
      "Params 10/42: {'leaf_size': 40, 'n_jobs': -1, 'n_neighbors': 20, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.05000216 0.         0.20754589]\n",
      "Outputdf: (10, 9)\n",
      "Output: {'f1_score': 0.41629955947136565, 'tn': 8870, 'fp': 39, 'fn': 491, 'tp': 189, 'roc_auc_score': 0.7344265217592257, 'average_precision_score': 0.39433260234390455, 'accuracy_score': 0.9447283345500053, 'name': 'KNeighborsRegressor-leaf_size40n_jobs-1n_neighbors20weightsdistance--tfidf-ngram'}\n",
      "Params 11/42: {'leaf_size': 40, 'n_jobs': -1, 'n_neighbors': 25, 'weights': 'uniform'}\n",
      "[0.   0.   0.   ... 0.04 0.   0.16]\n",
      "Outputdf: (11, 9)\n",
      "Output: {'f1_score': 0.18375499334221038, 'tn': 8907, 'fp': 2, 'fn': 611, 'tp': 69, 'roc_auc_score': 0.7291059767716719, 'average_precision_score': 0.33886689086396315, 'accuracy_score': 0.9360725831682136, 'name': 'KNeighborsRegressor-leaf_size40n_jobs-1n_neighbors25weightsuniform--tfidf-ngram'}\n",
      "Params 12/42: {'leaf_size': 40, 'n_jobs': -1, 'n_neighbors': 25, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.04004165 0.         0.16766689]\n",
      "Outputdf: (12, 9)\n",
      "Output: {'f1_score': 0.40929203539823, 'tn': 8870, 'fp': 39, 'fn': 495, 'tp': 185, 'roc_auc_score': 0.7342834905878391, 'average_precision_score': 0.3933333407923898, 'accuracy_score': 0.9443111899050995, 'name': 'KNeighborsRegressor-leaf_size40n_jobs-1n_neighbors25weightsdistance--tfidf-ngram'}\n",
      "Params 13/42: {'leaf_size': 40, 'n_jobs': -1, 'n_neighbors': 30, 'weights': 'uniform'}\n",
      "[0.1        0.13333333 0.13333333 ... 0.06666667 0.13333333 0.13333333]\n",
      "Outputdf: (13, 9)\n",
      "Output: {'f1_score': 0.1467391304347826, 'tn': 8907, 'fp': 2, 'fn': 626, 'tp': 54, 'roc_auc_score': 0.7282207186387856, 'average_precision_score': 0.2901214244293448, 'accuracy_score': 0.9345082907498174, 'name': 'KNeighborsRegressor-leaf_size40n_jobs-1n_neighbors30weightsuniform--tfidf-ngram'}\n",
      "Params 14/42: {'leaf_size': 40, 'n_jobs': -1, 'n_neighbors': 30, 'weights': 'distance'}\n",
      "[0.09983211 0.13333333 0.13333333 ... 0.06661261 0.13333333 0.14064299]\n",
      "Outputdf: (14, 9)\n",
      "Output: {'f1_score': 0.4039955604883463, 'tn': 8870, 'fp': 39, 'fn': 498, 'tp': 182, 'roc_auc_score': 0.7988836470720292, 'average_precision_score': 0.41307561357160455, 'accuracy_score': 0.9439983314214204, 'name': 'KNeighborsRegressor-leaf_size40n_jobs-1n_neighbors30weightsdistance--tfidf-ngram'}\n",
      "Params 15/42: {'leaf_size': 50, 'n_jobs': -1, 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "[0.         0.         0.         ... 0.33333333 0.         0.33333333]\n",
      "Outputdf: (15, 9)\n",
      "Output: {'f1_score': 0.38449367088607594, 'tn': 8568, 'fp': 341, 'fn': 437, 'tp': 243, 'roc_auc_score': 0.731059470594838, 'average_precision_score': 0.2834161936563818, 'accuracy_score': 0.9188653665658567, 'name': 'KNeighborsRegressor-leaf_size50n_jobs-1n_neighbors3weightsuniform--tfidf-ngram'}\n",
      "Params 16/42: {'leaf_size': 50, 'n_jobs': -1, 'n_neighbors': 3, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.32418799 0.         0.3195202 ]\n",
      "Outputdf: (16, 9)\n",
      "Output: {'f1_score': 0.4383975812547241, 'tn': 8556, 'fp': 353, 'fn': 390, 'tp': 290, 'roc_auc_score': 0.7355196001399774, 'average_precision_score': 0.34326111256533004, 'accuracy_score': 0.9225153822087809, 'name': 'KNeighborsRegressor-leaf_size50n_jobs-1n_neighbors3weightsdistance--tfidf-ngram'}\n",
      "Params 17/42: {'leaf_size': 50, 'n_jobs': -1, 'n_neighbors': 5, 'weights': 'uniform'}\n",
      "[0.  0.  0.  ... 0.2 0.  0.4]\n",
      "Outputdf: (17, 9)\n",
      "Output: {'f1_score': 0.35282837967401726, 'tn': 8730, 'fp': 179, 'fn': 496, 'tp': 184, 'roc_auc_score': 0.7299763788105882, 'average_precision_score': 0.3334241225627842, 'accuracy_score': 0.9296068411721764, 'name': 'KNeighborsRegressor-leaf_size50n_jobs-1n_neighbors5weightsuniform--tfidf-ngram'}\n",
      "Params 18/42: {'leaf_size': 50, 'n_jobs': -1, 'n_neighbors': 5, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.19706095 0.         0.38549401]\n",
      "Outputdf: (18, 9)\n",
      "Output: {'f1_score': 0.4338433843384339, 'tn': 8719, 'fp': 190, 'fn': 439, 'tp': 241, 'roc_auc_score': 0.735582986140915, 'average_precision_score': 0.40001522977368864, 'accuracy_score': 0.9344040045885911, 'name': 'KNeighborsRegressor-leaf_size50n_jobs-1n_neighbors5weightsdistance--tfidf-ngram'}\n",
      "Params 19/42: {'leaf_size': 50, 'n_jobs': -1, 'n_neighbors': 10, 'weights': 'uniform'}\n",
      "[0.  0.  0.  ... 0.1 0.  0.4]\n",
      "Outputdf: (19, 9)\n",
      "Output: {'f1_score': 0.31058823529411766, 'tn': 8871, 'fp': 38, 'fn': 548, 'tp': 132, 'roc_auc_score': 0.729430004687923, 'average_precision_score': 0.3415865995469962, 'accuracy_score': 0.9388883095213265, 'name': 'KNeighborsRegressor-leaf_size50n_jobs-1n_neighbors10weightsuniform--tfidf-ngram'}\n",
      "Params 20/42: {'leaf_size': 50, 'n_jobs': -1, 'n_neighbors': 10, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.09950816 0.         0.39584814]\n",
      "Outputdf: (20, 9)\n",
      "Output: {'f1_score': 0.45294725956566706, 'tn': 8841, 'fp': 68, 'fn': 461, 'tp': 219, 'roc_auc_score': 0.7349175981987811, 'average_precision_score': 0.40423774369595533, 'accuracy_score': 0.9448326207112316, 'name': 'KNeighborsRegressor-leaf_size50n_jobs-1n_neighbors10weightsdistance--tfidf-ngram'}\n",
      "Params 21/42: {'leaf_size': 50, 'n_jobs': -1, 'n_neighbors': 15, 'weights': 'uniform'}\n",
      "[0.         0.         0.         ... 0.06666667 0.         0.26666667]\n",
      "Outputdf: (21, 9)\n",
      "Output: {'f1_score': 0.2518891687657431, 'tn': 8895, 'fp': 14, 'fn': 580, 'tp': 100, 'roc_auc_score': 0.7293014994750847, 'average_precision_score': 0.3393128731235294, 'accuracy_score': 0.9380540202315153, 'name': 'KNeighborsRegressor-leaf_size50n_jobs-1n_neighbors15weightsuniform--tfidf-ngram'}\n",
      "Params 22/42: {'leaf_size': 50, 'n_jobs': -1, 'n_neighbors': 15, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.06655892 0.         0.27231511]\n",
      "Outputdf: (22, 9)\n",
      "Output: {'f1_score': 0.422077922077922, 'tn': 8860, 'fp': 49, 'fn': 485, 'tp': 195, 'roc_auc_score': 0.7345962938997578, 'average_precision_score': 0.3983552913286259, 'accuracy_score': 0.9443111899050995, 'name': 'KNeighborsRegressor-leaf_size50n_jobs-1n_neighbors15weightsdistance--tfidf-ngram'}\n",
      "Params 23/42: {'leaf_size': 50, 'n_jobs': -1, 'n_neighbors': 20, 'weights': 'uniform'}\n",
      "[0.   0.   0.   ... 0.05 0.   0.2 ]\n",
      "Outputdf: (23, 9)\n",
      "Output: {'f1_score': 0.21875, 'tn': 8905, 'fp': 4, 'fn': 596, 'tp': 84, 'roc_auc_score': 0.7292641116385943, 'average_precision_score': 0.339053709719031, 'accuracy_score': 0.9374283032641568, 'name': 'KNeighborsRegressor-leaf_size50n_jobs-1n_neighbors20weightsuniform--tfidf-ngram'}\n",
      "Params 24/42: {'leaf_size': 50, 'n_jobs': -1, 'n_neighbors': 20, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.05000216 0.         0.20754589]\n",
      "Outputdf: (24, 9)\n",
      "Output: {'f1_score': 0.41629955947136565, 'tn': 8870, 'fp': 39, 'fn': 491, 'tp': 189, 'roc_auc_score': 0.7344265217592257, 'average_precision_score': 0.39433260234390455, 'accuracy_score': 0.9447283345500053, 'name': 'KNeighborsRegressor-leaf_size50n_jobs-1n_neighbors20weightsdistance--tfidf-ngram'}\n",
      "Params 25/42: {'leaf_size': 50, 'n_jobs': -1, 'n_neighbors': 25, 'weights': 'uniform'}\n",
      "[0.   0.   0.   ... 0.04 0.   0.16]\n",
      "Outputdf: (25, 9)\n",
      "Output: {'f1_score': 0.18375499334221038, 'tn': 8907, 'fp': 2, 'fn': 611, 'tp': 69, 'roc_auc_score': 0.7291059767716719, 'average_precision_score': 0.33886689086396315, 'accuracy_score': 0.9360725831682136, 'name': 'KNeighborsRegressor-leaf_size50n_jobs-1n_neighbors25weightsuniform--tfidf-ngram'}\n",
      "Params 26/42: {'leaf_size': 50, 'n_jobs': -1, 'n_neighbors': 25, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.04004165 0.         0.16766689]\n",
      "Outputdf: (26, 9)\n",
      "Output: {'f1_score': 0.40929203539823, 'tn': 8870, 'fp': 39, 'fn': 495, 'tp': 185, 'roc_auc_score': 0.7342834905878391, 'average_precision_score': 0.3933333407923898, 'accuracy_score': 0.9443111899050995, 'name': 'KNeighborsRegressor-leaf_size50n_jobs-1n_neighbors25weightsdistance--tfidf-ngram'}\n",
      "Params 27/42: {'leaf_size': 50, 'n_jobs': -1, 'n_neighbors': 30, 'weights': 'uniform'}\n",
      "[0.1        0.13333333 0.13333333 ... 0.06666667 0.13333333 0.13333333]\n",
      "Outputdf: (27, 9)\n",
      "Output: {'f1_score': 0.1467391304347826, 'tn': 8907, 'fp': 2, 'fn': 626, 'tp': 54, 'roc_auc_score': 0.7282207186387856, 'average_precision_score': 0.2901214244293448, 'accuracy_score': 0.9345082907498174, 'name': 'KNeighborsRegressor-leaf_size50n_jobs-1n_neighbors30weightsuniform--tfidf-ngram'}\n",
      "Params 28/42: {'leaf_size': 50, 'n_jobs': -1, 'n_neighbors': 30, 'weights': 'distance'}\n",
      "[0.09983211 0.13333333 0.13333333 ... 0.06661261 0.13333333 0.14064299]\n",
      "Outputdf: (28, 9)\n",
      "Output: {'f1_score': 0.4039955604883463, 'tn': 8870, 'fp': 39, 'fn': 498, 'tp': 182, 'roc_auc_score': 0.7988836470720292, 'average_precision_score': 0.41307561357160455, 'accuracy_score': 0.9439983314214204, 'name': 'KNeighborsRegressor-leaf_size50n_jobs-1n_neighbors30weightsdistance--tfidf-ngram'}\n",
      "Params 29/42: {'leaf_size': 60, 'n_jobs': -1, 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "[0.         0.         0.         ... 0.33333333 0.         0.33333333]\n",
      "Outputdf: (29, 9)\n",
      "Output: {'f1_score': 0.38449367088607594, 'tn': 8568, 'fp': 341, 'fn': 437, 'tp': 243, 'roc_auc_score': 0.731059470594838, 'average_precision_score': 0.2834161936563818, 'accuracy_score': 0.9188653665658567, 'name': 'KNeighborsRegressor-leaf_size60n_jobs-1n_neighbors3weightsuniform--tfidf-ngram'}\n",
      "Params 30/42: {'leaf_size': 60, 'n_jobs': -1, 'n_neighbors': 3, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.32418799 0.         0.3195202 ]\n",
      "Outputdf: (30, 9)\n",
      "Output: {'f1_score': 0.4383975812547241, 'tn': 8556, 'fp': 353, 'fn': 390, 'tp': 290, 'roc_auc_score': 0.7355196001399774, 'average_precision_score': 0.34326111256533004, 'accuracy_score': 0.9225153822087809, 'name': 'KNeighborsRegressor-leaf_size60n_jobs-1n_neighbors3weightsdistance--tfidf-ngram'}\n",
      "Params 31/42: {'leaf_size': 60, 'n_jobs': -1, 'n_neighbors': 5, 'weights': 'uniform'}\n",
      "[0.  0.  0.  ... 0.2 0.  0.4]\n",
      "Outputdf: (31, 9)\n",
      "Output: {'f1_score': 0.35282837967401726, 'tn': 8730, 'fp': 179, 'fn': 496, 'tp': 184, 'roc_auc_score': 0.7299763788105882, 'average_precision_score': 0.3334241225627842, 'accuracy_score': 0.9296068411721764, 'name': 'KNeighborsRegressor-leaf_size60n_jobs-1n_neighbors5weightsuniform--tfidf-ngram'}\n",
      "Params 32/42: {'leaf_size': 60, 'n_jobs': -1, 'n_neighbors': 5, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.19706095 0.         0.38549401]\n",
      "Outputdf: (32, 9)\n",
      "Output: {'f1_score': 0.4338433843384339, 'tn': 8719, 'fp': 190, 'fn': 439, 'tp': 241, 'roc_auc_score': 0.735582986140915, 'average_precision_score': 0.40001522977368864, 'accuracy_score': 0.9344040045885911, 'name': 'KNeighborsRegressor-leaf_size60n_jobs-1n_neighbors5weightsdistance--tfidf-ngram'}\n",
      "Params 33/42: {'leaf_size': 60, 'n_jobs': -1, 'n_neighbors': 10, 'weights': 'uniform'}\n",
      "[0.  0.  0.  ... 0.1 0.  0.4]\n",
      "Outputdf: (33, 9)\n",
      "Output: {'f1_score': 0.31058823529411766, 'tn': 8871, 'fp': 38, 'fn': 548, 'tp': 132, 'roc_auc_score': 0.729430004687923, 'average_precision_score': 0.3415865995469962, 'accuracy_score': 0.9388883095213265, 'name': 'KNeighborsRegressor-leaf_size60n_jobs-1n_neighbors10weightsuniform--tfidf-ngram'}\n",
      "Params 34/42: {'leaf_size': 60, 'n_jobs': -1, 'n_neighbors': 10, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.09950816 0.         0.39584814]\n",
      "Outputdf: (34, 9)\n",
      "Output: {'f1_score': 0.45294725956566706, 'tn': 8841, 'fp': 68, 'fn': 461, 'tp': 219, 'roc_auc_score': 0.7349175981987811, 'average_precision_score': 0.40423774369595533, 'accuracy_score': 0.9448326207112316, 'name': 'KNeighborsRegressor-leaf_size60n_jobs-1n_neighbors10weightsdistance--tfidf-ngram'}\n",
      "Params 35/42: {'leaf_size': 60, 'n_jobs': -1, 'n_neighbors': 15, 'weights': 'uniform'}\n",
      "[0.         0.         0.         ... 0.06666667 0.         0.26666667]\n",
      "Outputdf: (35, 9)\n",
      "Output: {'f1_score': 0.2518891687657431, 'tn': 8895, 'fp': 14, 'fn': 580, 'tp': 100, 'roc_auc_score': 0.7293014994750847, 'average_precision_score': 0.3393128731235294, 'accuracy_score': 0.9380540202315153, 'name': 'KNeighborsRegressor-leaf_size60n_jobs-1n_neighbors15weightsuniform--tfidf-ngram'}\n",
      "Params 36/42: {'leaf_size': 60, 'n_jobs': -1, 'n_neighbors': 15, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.06655892 0.         0.27231511]\n",
      "Outputdf: (36, 9)\n",
      "Output: {'f1_score': 0.422077922077922, 'tn': 8860, 'fp': 49, 'fn': 485, 'tp': 195, 'roc_auc_score': 0.7345962938997578, 'average_precision_score': 0.3983552913286259, 'accuracy_score': 0.9443111899050995, 'name': 'KNeighborsRegressor-leaf_size60n_jobs-1n_neighbors15weightsdistance--tfidf-ngram'}\n",
      "Params 37/42: {'leaf_size': 60, 'n_jobs': -1, 'n_neighbors': 20, 'weights': 'uniform'}\n",
      "[0.   0.   0.   ... 0.05 0.   0.2 ]\n",
      "Outputdf: (37, 9)\n",
      "Output: {'f1_score': 0.21875, 'tn': 8905, 'fp': 4, 'fn': 596, 'tp': 84, 'roc_auc_score': 0.7292641116385943, 'average_precision_score': 0.339053709719031, 'accuracy_score': 0.9374283032641568, 'name': 'KNeighborsRegressor-leaf_size60n_jobs-1n_neighbors20weightsuniform--tfidf-ngram'}\n",
      "Params 38/42: {'leaf_size': 60, 'n_jobs': -1, 'n_neighbors': 20, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.05000216 0.         0.20754589]\n",
      "Outputdf: (38, 9)\n",
      "Output: {'f1_score': 0.41629955947136565, 'tn': 8870, 'fp': 39, 'fn': 491, 'tp': 189, 'roc_auc_score': 0.7344265217592257, 'average_precision_score': 0.39433260234390455, 'accuracy_score': 0.9447283345500053, 'name': 'KNeighborsRegressor-leaf_size60n_jobs-1n_neighbors20weightsdistance--tfidf-ngram'}\n",
      "Params 39/42: {'leaf_size': 60, 'n_jobs': -1, 'n_neighbors': 25, 'weights': 'uniform'}\n",
      "[0.   0.   0.   ... 0.04 0.   0.16]\n",
      "Outputdf: (39, 9)\n",
      "Output: {'f1_score': 0.18375499334221038, 'tn': 8907, 'fp': 2, 'fn': 611, 'tp': 69, 'roc_auc_score': 0.7291059767716719, 'average_precision_score': 0.33886689086396315, 'accuracy_score': 0.9360725831682136, 'name': 'KNeighborsRegressor-leaf_size60n_jobs-1n_neighbors25weightsuniform--tfidf-ngram'}\n",
      "Params 40/42: {'leaf_size': 60, 'n_jobs': -1, 'n_neighbors': 25, 'weights': 'distance'}\n",
      "[0.         0.         0.         ... 0.04004165 0.         0.16766689]\n",
      "Outputdf: (40, 9)\n",
      "Output: {'f1_score': 0.40929203539823, 'tn': 8870, 'fp': 39, 'fn': 495, 'tp': 185, 'roc_auc_score': 0.7342834905878391, 'average_precision_score': 0.3933333407923898, 'accuracy_score': 0.9443111899050995, 'name': 'KNeighborsRegressor-leaf_size60n_jobs-1n_neighbors25weightsdistance--tfidf-ngram'}\n",
      "Params 41/42: {'leaf_size': 60, 'n_jobs': -1, 'n_neighbors': 30, 'weights': 'uniform'}\n",
      "[0.1        0.13333333 0.13333333 ... 0.06666667 0.13333333 0.13333333]\n",
      "Outputdf: (41, 9)\n",
      "Output: {'f1_score': 0.1467391304347826, 'tn': 8907, 'fp': 2, 'fn': 626, 'tp': 54, 'roc_auc_score': 0.7282207186387856, 'average_precision_score': 0.2901214244293448, 'accuracy_score': 0.9345082907498174, 'name': 'KNeighborsRegressor-leaf_size60n_jobs-1n_neighbors30weightsuniform--tfidf-ngram'}\n",
      "Params 42/42: {'leaf_size': 60, 'n_jobs': -1, 'n_neighbors': 30, 'weights': 'distance'}\n",
      "[0.09983211 0.13333333 0.13333333 ... 0.06661261 0.13333333 0.14064299]\n",
      "Outputdf: (42, 9)\n",
      "Output: {'f1_score': 0.4039955604883463, 'tn': 8870, 'fp': 39, 'fn': 498, 'tp': 182, 'roc_auc_score': 0.7988836470720292, 'average_precision_score': 0.41307561357160455, 'accuracy_score': 0.9439983314214204, 'name': 'KNeighborsRegressor-leaf_size60n_jobs-1n_neighbors30weightsdistance--tfidf-ngram'}\n",
      "Tuning ended\n",
      "===============\n",
      "=====================\n",
      "==SVR==\n",
      "=====================\n",
      "Starting tuning of SVR with data labeled: -tfidf-ngram\n",
      "===============\n",
      "Params 1/32: {'C': 1, 'epsilon': 0.1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "[0.45403127 0.47575024 0.4781422  ... 0.46565974 0.49038547 0.4960149 ]\n",
      "Outputdf: (43, 9)\n",
      "Output: {'f1_score': 0.49626651349798956, 'tn': 8280, 'fp': 629, 'fn': 248, 'tp': 432, 'roc_auc_score': 0.912527896443121, 'average_precision_score': 0.552330775843601, 'accuracy_score': 0.9085410366044426, 'name': 'SVR-C1epsilon0.1gamma0.001kernelrbf--tfidf-ngram'}\n",
      "Params 2/32: {'C': 1, 'epsilon': 0.1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "[0.49539817 0.49757379 0.49781332 ... 0.49656328 0.49903969 0.49960361]\n",
      "Outputdf: (44, 9)\n",
      "Output: {'f1_score': 0.49541284403669733, 'tn': 8277, 'fp': 632, 'fn': 248, 'tp': 432, 'roc_auc_score': 0.9125209635992684, 'average_precision_score': 0.5522823672801185, 'accuracy_score': 0.9082281781207634, 'name': 'SVR-C1epsilon0.1gamma0.0001kernelrbf--tfidf-ngram'}\n",
      "Params 3/32: {'C': 1, 'epsilon': 0.01, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "[0.45403127 0.47575024 0.4781422  ... 0.46565974 0.49038547 0.4960149 ]\n",
      "Outputdf: (45, 9)\n",
      "Output: {'f1_score': 0.49626651349798956, 'tn': 8280, 'fp': 629, 'fn': 248, 'tp': 432, 'roc_auc_score': 0.912527896443121, 'average_precision_score': 0.552330775843601, 'accuracy_score': 0.9085410366044426, 'name': 'SVR-C1epsilon0.01gamma0.001kernelrbf--tfidf-ngram'}\n",
      "Params 4/32: {'C': 1, 'epsilon': 0.01, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "[0.49539817 0.49757379 0.49781332 ... 0.49656328 0.49903969 0.49960361]\n",
      "Outputdf: (46, 9)\n",
      "Output: {'f1_score': 0.49541284403669733, 'tn': 8277, 'fp': 632, 'fn': 248, 'tp': 432, 'roc_auc_score': 0.9125209635992684, 'average_precision_score': 0.5522823672801185, 'accuracy_score': 0.9082281781207634, 'name': 'SVR-C1epsilon0.01gamma0.0001kernelrbf--tfidf-ngram'}\n",
      "Params 5/32: {'C': 1, 'epsilon': 0.001, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "[0.45403127 0.47575024 0.4781422  ... 0.46565974 0.49038547 0.4960149 ]\n",
      "Outputdf: (47, 9)\n",
      "Output: {'f1_score': 0.49626651349798956, 'tn': 8280, 'fp': 629, 'fn': 248, 'tp': 432, 'roc_auc_score': 0.912527896443121, 'average_precision_score': 0.552330775843601, 'accuracy_score': 0.9085410366044426, 'name': 'SVR-C1epsilon0.001gamma0.001kernelrbf--tfidf-ngram'}\n",
      "Params 6/32: {'C': 1, 'epsilon': 0.001, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "[0.49539817 0.49757379 0.49781332 ... 0.49656328 0.49903969 0.49960361]\n",
      "Outputdf: (48, 9)\n",
      "Output: {'f1_score': 0.49541284403669733, 'tn': 8277, 'fp': 632, 'fn': 248, 'tp': 432, 'roc_auc_score': 0.9125209635992684, 'average_precision_score': 0.5522823672801185, 'accuracy_score': 0.9082281781207634, 'name': 'SVR-C1epsilon0.001gamma0.0001kernelrbf--tfidf-ngram'}\n",
      "Params 7/32: {'C': 1, 'epsilon': 0.0001, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "[0.45403127 0.47575024 0.4781422  ... 0.46565974 0.49038547 0.4960149 ]\n",
      "Outputdf: (49, 9)\n",
      "Output: {'f1_score': 0.49626651349798956, 'tn': 8280, 'fp': 629, 'fn': 248, 'tp': 432, 'roc_auc_score': 0.912527896443121, 'average_precision_score': 0.552330775843601, 'accuracy_score': 0.9085410366044426, 'name': 'SVR-C1epsilon0.0001gamma0.001kernelrbf--tfidf-ngram'}\n",
      "Params 8/32: {'C': 1, 'epsilon': 0.0001, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "[0.49539817 0.49757379 0.49781332 ... 0.49656328 0.49903969 0.49960361]\n",
      "Outputdf: (50, 9)\n",
      "Output: {'f1_score': 0.49541284403669733, 'tn': 8277, 'fp': 632, 'fn': 248, 'tp': 432, 'roc_auc_score': 0.9125209635992684, 'average_precision_score': 0.5522823672801185, 'accuracy_score': 0.9082281781207634, 'name': 'SVR-C1epsilon0.0001gamma0.0001kernelrbf--tfidf-ngram'}\n",
      "Params 9/32: {'C': 10, 'epsilon': 0.1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "[0.16318487 0.28421211 0.29754038 ... 0.19595084 0.41538635 0.45745858]\n",
      "Outputdf: (51, 9)\n",
      "Output: {'f1_score': 0.5221445221445222, 'tn': 8321, 'fp': 588, 'fn': 232, 'tp': 448, 'roc_auc_score': 0.9191329158220702, 'average_precision_score': 0.5750882863688515, 'accuracy_score': 0.9144853477943476, 'name': 'SVR-C10epsilon0.1gamma0.001kernelrbf--tfidf-ngram'}\n",
      "Params 10/32: {'C': 10, 'epsilon': 0.1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "[0.45398167 0.47573789 0.4781332  ... 0.46563282 0.4903969  0.49603615]\n",
      "Outputdf: (52, 9)\n",
      "Output: {'f1_score': 0.49541284403669733, 'tn': 8277, 'fp': 632, 'fn': 248, 'tp': 432, 'roc_auc_score': 0.9125209635992684, 'average_precision_score': 0.5522823672801185, 'accuracy_score': 0.9082281781207634, 'name': 'SVR-C10epsilon0.1gamma0.0001kernelrbf--tfidf-ngram'}\n",
      "Params 11/32: {'C': 10, 'epsilon': 0.01, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "[0.13380603 0.26241704 0.28180113 ... 0.17395765 0.40091156 0.44587146]\n",
      "Outputdf: (53, 9)\n",
      "Output: {'f1_score': 0.527363184079602, 'tn': 8405, 'fp': 504, 'fn': 256, 'tp': 424, 'roc_auc_score': 0.9169287666800922, 'average_precision_score': 0.5719213618458314, 'accuracy_score': 0.920742517467932, 'name': 'SVR-C10epsilon0.01gamma0.001kernelrbf--tfidf-ngram'}\n",
      "Params 12/32: {'C': 10, 'epsilon': 0.01, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "[0.45398167 0.47573789 0.4781332  ... 0.46563282 0.4903969  0.49603615]\n",
      "Outputdf: (54, 9)\n",
      "Output: {'f1_score': 0.49541284403669733, 'tn': 8277, 'fp': 632, 'fn': 248, 'tp': 432, 'roc_auc_score': 0.9125209635992684, 'average_precision_score': 0.5522823672801185, 'accuracy_score': 0.9082281781207634, 'name': 'SVR-C10epsilon0.01gamma0.0001kernelrbf--tfidf-ngram'}\n",
      "Params 13/32: {'C': 10, 'epsilon': 0.001, 'gamma': 0.001, 'kernel': 'rbf'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 75\u001B[0m\n\u001B[0;32m     68\u001B[0m             \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m=====================\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     70\u001B[0m        \u001B[38;5;66;03m# df, df_tfidf, tr = load_and_clean_data('data/train.csv', rename_dict={'tweet':'text', 'label':'output'})\u001B[39;00m\n\u001B[0;32m     71\u001B[0m         \u001B[38;5;66;03m#X_train, X_valid, y_train, y_valid = train_test_split(df_tfidf, df['output'],test_size=0.3,random_state=random_state)\u001B[39;00m\n\u001B[0;32m     72\u001B[0m        \u001B[38;5;66;03m# X_train_balanced, y_train_balanced = balance_data(X_train, y_train)\u001B[39;00m\n\u001B[1;32m---> 75\u001B[0m         output, output_df \u001B[38;5;241m=\u001B[39m \u001B[43m__model_tuning__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_balanced\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_balanced\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_valid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_valid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_grid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_df\u001B[49m\u001B[43m \u001B[49m\u001B[43m,\u001B[49m\u001B[43mdata_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvectorizer_label\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     76\u001B[0m output_df\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mout/results/Results.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     77\u001B[0m     \u001B[38;5;66;03m#output2 = __model_tuning__(X_train, y_train, X_valid, y_valid, model, param_grid, 'lin_reg_1' ,'unba', verbose=verbose)\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[11], line 15\u001B[0m, in \u001B[0;36m__model_tuning__\u001B[1;34m(X_train, y_train, X_valid, y_valid, model_type, params, output_df, data_label, verbose, plot)\u001B[0m\n\u001B[0;32m     13\u001B[0m model_output \u001B[38;5;241m=\u001B[39m _model_output\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m$\u001B[39m\u001B[38;5;124m'\u001B[39m,param_string)\n\u001B[0;32m     14\u001B[0m model \u001B[38;5;241m=\u001B[39m fit_model(X_train, y_train, model_type, params\u001B[38;5;241m=\u001B[39mp, model_output\u001B[38;5;241m=\u001B[39mmodel_output)\n\u001B[1;32m---> 15\u001B[0m predictions, model_class \u001B[38;5;241m=\u001B[39m \u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_valid\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(predictions)\n\u001B[0;32m     17\u001B[0m output, output_df \u001B[38;5;241m=\u001B[39m metrics(predictions, y_valid, output_name\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m$\u001B[39m\u001B[38;5;124m'\u001B[39m,param_string),plot\u001B[38;5;241m=\u001B[39mplot, model_class\u001B[38;5;241m=\u001B[39mmodel_class, output_df\u001B[38;5;241m=\u001B[39moutput_df)\n",
      "Cell \u001B[1;32mIn[8], line 49\u001B[0m, in \u001B[0;36mpredict\u001B[1;34m(model, data)\u001B[0m\n\u001B[0;32m     47\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict_proba(data)\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 49\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m predictions, model_type\n",
      "File \u001B[1;32mW:\\diploma_thesis\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:435\u001B[0m, in \u001B[0;36mBaseLibSVM.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    433\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_for_predict(X)\n\u001B[0;32m    434\u001B[0m predict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sparse_predict \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sparse \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dense_predict\n\u001B[1;32m--> 435\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mW:\\diploma_thesis\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:454\u001B[0m, in \u001B[0;36mBaseLibSVM._dense_predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    446\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    447\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX.shape[1] = \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m should be equal to \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    448\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe number of samples at training time\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    449\u001B[0m             \u001B[38;5;241m%\u001B[39m (X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape_fit_[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m    450\u001B[0m         )\n\u001B[0;32m    452\u001B[0m svm_type \u001B[38;5;241m=\u001B[39m LIBSVM_IMPL\u001B[38;5;241m.\u001B[39mindex(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_impl)\n\u001B[1;32m--> 454\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlibsvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    455\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    456\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msupport_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msupport_vectors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    458\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_n_support\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    459\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dual_coef_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_intercept_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    461\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_probA\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    462\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_probB\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    463\u001B[0m \u001B[43m    \u001B[49m\u001B[43msvm_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msvm_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    464\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkernel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkernel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    465\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdegree\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdegree\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    466\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcoef0\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcoef0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    467\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgamma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gamma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    468\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcache_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    469\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    ( KNeighborsRegressor, {\n",
    "            'n_neighbors' : [3, 5, 10, 15, 20, 25, 30],\n",
    "            'weights' : ['uniform', 'distance'],\n",
    "            'leaf_size' : [ 40, 50, 60],\n",
    "            'n_jobs' : [-1]\n",
    "        }),\n",
    "    ( SVR, {\n",
    "        'kernel' : ['rbf'],\n",
    "        'C' : [1, 10, 100, 1000],\n",
    "        'gamma' : [0.001, 0.0001],\n",
    "        'epsilon' : [0.1, 0.01, 0.001, 0.0001],\n",
    "    }),\n",
    "    (DecisionTreeRegressor,{\n",
    "        'criterion' : ['squared_error', 'poisson'],\n",
    "        'splitter' : ['best'],\n",
    "        'max_depth' : [None, 10, 20, 30, 40, 50],\n",
    "        'min_samples_split' : [2, 5],\n",
    "    }),\n",
    "    (LogisticRegression, {\n",
    "        'penalty': ['none', 'l2'],\n",
    "        'class_weight': [None,'balanced'],\n",
    "        'n_jobs': [-1],\n",
    "        'solver': ['newton-cholesky']\n",
    "    }),\n",
    "    (BernoulliNB, {\n",
    "        'alpha' : np.arange(0.0, 1.0, 0.2),\n",
    "        'fit_prior': [True, False],\n",
    "        'binarize': [None]\n",
    "\n",
    "    }),\n",
    "    (MLPClassifier, {\n",
    "        'hidden_layer_sizes': [(100,), (100, 100), (50, 50, 50), (50,)],\n",
    "        'activation': ['identity', 'logistic'],\n",
    "        'alpha': [0.0001, 0.001]\n",
    "    }),\n",
    "    (ComplementNB,{\n",
    "        'alpha' : np.arange(0.0, 1.0, 0.2),\n",
    "        'fit_prior': [True, False],\n",
    "        'norm': [True, False],\n",
    "    })\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "vectorizers = [\n",
    "    (TfidfDataTransformer(vectorizer=TfidfVectorizer(max_features=2000, stop_words='english')), '-tfidf'),\n",
    "    (TfidfDataTransformer(vectorizer=TfidfVectorizer(max_features=2000, stop_words='english', ngram_range=(1,2))), '-tfidf-ngram'),\n",
    "    (TfidfDataTransformer(vectorizer=TfidfVectorizer(max_features=2000, stop_words='english', ngram_range=(1,3))), '-tfidf-ngram3'),\n",
    "    (BagOfWordsTransformer(vectorizer=CountVectorizer(max_features=2000, stop_words='english')), 'bow'),\n",
    "    (BagOfWordsTransformer(vectorizer=CountVectorizer(max_features=2000, stop_words='english', ngram_range=(1,2))), '-bow-ngram'),\n",
    "    (BagOfWordsTransformer(vectorizer=CountVectorizer(max_features=2000, stop_words='english', ngram_range=(1,3))), '-bow-ngram3'),\n",
    "]\n",
    "\n",
    "output_df = None\n",
    "for vectorizer, vectorizer_label in vectorizers[1:2]:\n",
    "    if verbose:\n",
    "        print('=====================')\n",
    "        print(f'=={vectorizer.__class__.__name__}==')\n",
    "        print('=====================')\n",
    "    df, df_tfidf, tr_old = load_and_clean_data('data/train.csv',vectorizer_output=f'models/vecs/{vectorizer_label}.sav', tr=vectorizer, rename_dict={'tweet':'text', 'label':'output'})\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(df_tfidf, df['output'],test_size=0.3,random_state=random_state)\n",
    "    X_train_balanced, y_train_balanced = balance_data(X_train, y_train)\n",
    "    for model, param_grid in models[:2]:\n",
    "        if verbose:\n",
    "            print('=====================')\n",
    "            print(f'=={model.__name__}==')\n",
    "            print('=====================')\n",
    "\n",
    "       # df, df_tfidf, tr = load_and_clean_data('data/train.csv', rename_dict={'tweet':'text', 'label':'output'})\n",
    "        #X_train, X_valid, y_train, y_valid = train_test_split(df_tfidf, df['output'],test_size=0.3,random_state=random_state)\n",
    "       # X_train_balanced, y_train_balanced = balance_data(X_train, y_train)\n",
    "\n",
    "\n",
    "        output, output_df = __model_tuning__(X_train_balanced, y_train_balanced, X_valid, y_valid, model, param_grid, output_df ,data_label=vectorizer_label, verbose=verbose)\n",
    "output_df.to_csv(f'out/results/Results.csv')\n",
    "    #output2 = __model_tuning__(X_train, y_train, X_valid, y_valid, model, param_grid, 'lin_reg_1' ,'unba', verbose=verbose)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_df.to_csv(f'out/results/Results.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model1_grid= {\n",
    "    'penalty': ['none', 'l2'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'n_jobs': [-1],\n",
    "    'solver': ['newton-cholesky'],\n",
    "    'max_iter': [500]\n",
    "}\n",
    "model1 = LogisticRegression\n",
    "df, df_tfidf, tr = load_and_clean_data('data/train.csv', rename_dict={'tweet':'text', 'label':'output'})\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(df_tfidf, df['output'],test_size=0.3,random_state=random_state)\n",
    "\n",
    "balancers = [TomekLinks(sampling_strategy='majority'), TomekLinks(sampling_strategy='not minority'),  RandomUnderSampler(), SMOTE(sampling_strategy='minority'),CondensedNearestNeighbour()]\n",
    "results = []\n",
    "for b in balancers[:3]:\n",
    "    X_train_balanced, y_train_balanced = balance_data(X_train, y_train, b)\n",
    "    output = __model_tuning__(X_train_balanced, y_train_balanced, X_valid, y_valid, model1, model1_grid, 'lin_reg_1' ,'bal', verbose=False)\n",
    "\n",
    "    results.append(f'Balancer {str(b)} : {max(output, key=lambda x: x[\"roc_auc_score\"])}')\n",
    "\n",
    "print('========')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancer TomekLinks(sampling_strategy='majority') : {'f1_score': 0.389544150462225, 'confusion_matrix': array([[7063, 1846],\n",
      "       [  69,  611]], dtype=int64), 'roc_auc_score': 0.9374362013297856, 'average_precision_score': 0.6739797660178359}\n",
      "Balancer TomekLinks(sampling_strategy='not minority') : {'f1_score': 0.389544150462225, 'confusion_matrix': array([[7063, 1846],\n",
      "       [  69,  611]], dtype=int64), 'roc_auc_score': 0.9374362013297856, 'average_precision_score': 0.6739797660178359}\n",
      "Balancer RandomUnderSampler() : {'f1_score': 0.24647364513734224, 'confusion_matrix': array([[4865, 4044],\n",
      "       [  16,  664]], dtype=int64), 'roc_auc_score': 0.9288538358434629, 'average_precision_score': 0.6292684522647592}\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    print(r)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.73975098, 0.26024902]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test = 'bad bad i hate this ugly thingy libtard'\n",
    "test2 = 'i like this its beautiful, nice job'\n",
    "m = 'LogisticRegression-class_weightNonen_jobs-1penaltynonesolvernewton-cholesky-bal-1680105414.123568.sav'\n",
    "analyze_sentence(test2, model_file=f'models/{m}', vectorizer_file='models/vectorizer.sav')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(X_train_balanced, y_train_balanced))\n\u001B[0;32m     11\u001B[0m valid_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(X_valid, y_valid))\n\u001B[1;32m---> 12\u001B[0m \u001B[43mclassifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m nltk\u001B[38;5;241m.\u001B[39mclassify\u001B[38;5;241m.\u001B[39maccuracy(classifier, valid_data)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2%\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mW:\\diploma_thesis\\venv\\lib\\site-packages\\nltk\\classify\\scikitlearn.py:112\u001B[0m, in \u001B[0;36mSklearnClassifier.train\u001B[1;34m(self, labeled_featuresets)\u001B[0m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;124;03mTrain (fit) the scikit-learn estimator.\u001B[39;00m\n\u001B[0;32m    105\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;124;03m    numbers, booleans or strings.\u001B[39;00m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    111\u001B[0m X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mlabeled_featuresets))\n\u001B[1;32m--> 112\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_vectorizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    113\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_encoder\u001B[38;5;241m.\u001B[39mfit_transform(y)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clf\u001B[38;5;241m.\u001B[39mfit(X, y)\n",
      "File \u001B[1;32mW:\\diploma_thesis\\venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[1;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 142\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    143\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    144\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[0;32m    145\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m    146\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    147\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[0;32m    148\u001B[0m         )\n",
      "File \u001B[1;32mW:\\diploma_thesis\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\_dict_vectorizer.py:312\u001B[0m, in \u001B[0;36mDictVectorizer.fit_transform\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    289\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Learn a list of feature name -> indices mappings and transform X.\u001B[39;00m\n\u001B[0;32m    290\u001B[0m \n\u001B[0;32m    291\u001B[0m \u001B[38;5;124;03mLike fit(X) followed by transform(X), but does not require\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    309\u001B[0m \u001B[38;5;124;03m    Feature vectors; always 2-d.\u001B[39;00m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m--> 312\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfitting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mW:\\diploma_thesis\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\_dict_vectorizer.py:222\u001B[0m, in \u001B[0;36mDictVectorizer._transform\u001B[1;34m(self, X, fitting)\u001B[0m\n\u001B[0;32m    219\u001B[0m \u001B[38;5;66;03m# collect all the possible feature names and build sparse matrix at\u001B[39;00m\n\u001B[0;32m    220\u001B[0m \u001B[38;5;66;03m# same time\u001B[39;00m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m X:\n\u001B[1;32m--> 222\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f, v \u001B[38;5;129;01min\u001B[39;00m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m():\n\u001B[0;32m    223\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(v, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    224\u001B[0m             feature_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (f, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseparator, v)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'int' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "df, df_tfidf, tr = load_and_clean_data('data/train.csv', rename_dict={'tweet':'text', 'label':'output'})\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(df_tfidf, df['output'],test_size=0.3,random_state=random_state)\n",
    "X_train_balanced, y_train_balanced = balance_data(X_train, y_train)\n",
    "\n",
    "\n",
    "classifier = nltk.classify.SklearnClassifier(LogisticRegression())\n",
    "# Transform two arrays (X_train_balanced, y_train_balanced) into a list of tuples\n",
    "data = list(zip(X_train_balanced, y_train_balanced))\n",
    "valid_data = list(zip(X_valid, y_valid))\n",
    "classifier.train(data)\n",
    "accuracy = nltk.classify.accuracy(classifier, valid_data)\n",
    "print(f\"{accuracy:.2%}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score\n"
     ]
    },
    {
     "data": {
      "text/plain": "     roc_auc_score                                               name\n147       0.936441  ComplementNB-alpha0.8fit_priorFalsenormFalse--...\n145       0.936441  ComplementNB-alpha0.8fit_priorTruenormFalse--t...\n111       0.936394  BernoulliNB-alpha0.8binarizeNonefit_priorFalse...\n110       0.936389  BernoulliNB-alpha0.8binarizeNonefit_priorTrue-...\n143       0.936195  ComplementNB-alpha0.6000000000000001fit_priorF...\n141       0.936195  ComplementNB-alpha0.6000000000000001fit_priorT...\n109       0.936161  BernoulliNB-alpha0.6000000000000001binarizeNon...\n108       0.936161  BernoulliNB-alpha0.6000000000000001binarizeNon...\n146       0.935747  ComplementNB-alpha0.8fit_priorFalsenormTrue--t...\n144       0.935747  ComplementNB-alpha0.8fit_priorTruenormTrue--tfidf",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>roc_auc_score</th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>147</th>\n      <td>0.936441</td>\n      <td>ComplementNB-alpha0.8fit_priorFalsenormFalse--...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>0.936441</td>\n      <td>ComplementNB-alpha0.8fit_priorTruenormFalse--t...</td>\n    </tr>\n    <tr>\n      <th>111</th>\n      <td>0.936394</td>\n      <td>BernoulliNB-alpha0.8binarizeNonefit_priorFalse...</td>\n    </tr>\n    <tr>\n      <th>110</th>\n      <td>0.936389</td>\n      <td>BernoulliNB-alpha0.8binarizeNonefit_priorTrue-...</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>0.936195</td>\n      <td>ComplementNB-alpha0.6000000000000001fit_priorF...</td>\n    </tr>\n    <tr>\n      <th>141</th>\n      <td>0.936195</td>\n      <td>ComplementNB-alpha0.6000000000000001fit_priorT...</td>\n    </tr>\n    <tr>\n      <th>109</th>\n      <td>0.936161</td>\n      <td>BernoulliNB-alpha0.6000000000000001binarizeNon...</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>0.936161</td>\n      <td>BernoulliNB-alpha0.6000000000000001binarizeNon...</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>0.935747</td>\n      <td>ComplementNB-alpha0.8fit_priorFalsenormTrue--t...</td>\n    </tr>\n    <tr>\n      <th>144</th>\n      <td>0.935747</td>\n      <td>ComplementNB-alpha0.8fit_priorTruenormTrue--tfidf</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score\n",
      "200    SVR-C10epsilon0.01gamma0.001kernelrbf--tfidf-n...\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load Results.csv as dataframe\n",
    "df = pd.read_csv('out/results/Results_complete.csv')\n",
    "# Sort by roc_auc_score\n",
    "df = df.sort_values(by=['roc_auc_score'], ascending=False)\n",
    "# Print top 10\n",
    "print(f'ROC AUC Score')\n",
    "display(df[['roc_auc_score', 'name']].head(10))\n",
    "# Sort by f1_score\n",
    "df = df.sort_values(by=['f1_score'], ascending=False)\n",
    "# Print top 10\n",
    "print(f'F1 Score')\n",
    "print(df[['f1_score', 'name']].head(1)['name'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1_win = 'SVR-C10epsilon0.01gamma0.001kernelrbf--tfidf-ngram'\n",
    "acc_win = 'KNeighborsRegressor-leaf_size40n_jobs-1n_neighbors10weightsdistance--tfidf-ngram'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df, df_tfidf, tr_old = load_and_clean_data('data/train.csv',vectorizer_output=f'models/vecs/v1.sav', tr=vectorizer, rename_dict={'tweet':'text', 'label':'output'})\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(df_tfidf, df['output'],test_size=0.3,random_state=random_state)\n",
    "X_train_balanced, y_train_balanced = balance_data(X_train, y_train)\n",
    "for model, param_grid in models:\n",
    "    if verbose:\n",
    "        print('=====================')\n",
    "        print(f'=={model.__name__}==')\n",
    "        print('=====================')\n",
    "\n",
    "    # df, df_tfidf, tr = load_and_clean_data('data/train.csv', rename_dict={'tweet':'text', 'label':'output'})\n",
    "    #X_train, X_valid, y_train, y_valid = train_test_split(df_tfidf, df['output'],test_size=0.3,random_state=random_state)\n",
    "    # X_train_balanced, y_train_balanced = balance_data(X_train, y_train)\n",
    "\n",
    "\n",
    "    output, output_df = __model_tuning__(X_train_balanced, y_train_balanced, X_valid, y_valid, model, param_grid, output_df ,data_label=vectorizer_label, verbose=verbose)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
