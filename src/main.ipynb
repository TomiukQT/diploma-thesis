{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_t = df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   id  label                                              tweet\n0   1      0   @user when a father is dysfunctional and is s...\n1   2      0  @user @user thanks for #lyft credit i can't us...\n2   3      0                                bihday your majesty\n3   4      0  #model   i love u take with u all the time in ...\n4   5      0             factsguide: society now    #motivation",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>@user when a father is dysfunctional and is s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>@user @user thanks for #lyft credit i can't us...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n      <td>bihday your majesty</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>#model   i love u take with u all the time in ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>factsguide: society now    #motivation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   id  label                                              tweet  \\\n0   1      0   @user when a father is dysfunctional and is s...   \n1   2      0  @user @user thanks for #lyft credit i can't us...   \n2   3      0                                bihday your majesty   \n3   4      0  #model   i love u take with u all the time in ...   \n4   5      0             factsguide: society now    #motivation   \n\n                                         clean_tweet  \n0  when father dysfunctional and selfish drags hi...  \n1  thanks for #lyft credit can use cause they don...  \n2                                bihday your majesty  \n3                 #model love take with all the time  \n4                 factsguide society now #motivation  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n      <th>tweet</th>\n      <th>clean_tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>@user when a father is dysfunctional and is s...</td>\n      <td>when father dysfunctional and selfish drags hi...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>@user @user thanks for #lyft credit i can't us...</td>\n      <td>thanks for #lyft credit can use cause they don...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n      <td>bihday your majesty</td>\n      <td>bihday your majesty</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>#model   i love u take with u all the time in ...</td>\n      <td>#model love take with all the time</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>factsguide: society now    #motivation</td>\n      <td>factsguide society now #motivation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['clean_tweet'] = np.vectorize(data_cleaner.clean_mentions)(df_train['tweet'])\n",
    "df_train['clean_tweet'] = df_train['clean_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "df_train['clean_tweet'] = df_train['clean_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   id  label                                              tweet  \\\n0   1      0   @user when a father is dysfunctional and is s...   \n1   2      0  @user @user thanks for #lyft credit i can't us...   \n2   3      0                                bihday your majesty   \n3   4      0  #model   i love u take with u all the time in ...   \n4   5      0             factsguide: society now    #motivation   \n\n                                         clean_tweet  \n0  when father dysfunct and selfish drag hi kid i...  \n1  thank for #lyft credit can use caus they don o...  \n2                                bihday your majesti  \n3                 #model love take with all the time  \n4                       factsguid societi now #motiv  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n      <th>tweet</th>\n      <th>clean_tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>@user when a father is dysfunctional and is s...</td>\n      <td>when father dysfunct and selfish drag hi kid i...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>@user @user thanks for #lyft credit i can't us...</td>\n      <td>thank for #lyft credit can use caus they don o...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n      <td>bihday your majesty</td>\n      <td>bihday your majesti</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>#model   i love u take with u all the time in ...</td>\n      <td>#model love take with all the time</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>factsguide: society now    #motivation</td>\n      <td>factsguid societi now #motiv</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tr= TfidfDataTransformer()\n",
    "\n",
    "df_train['clean_tweet'] = tr.stemming(df_train['clean_tweet'])\n",
    "display(df_train.head())\n",
    "df_train['clean_tweet'].to_csv('../models/model_data.csv', sep=',')\n",
    "tr.vectorizer_fit(df_train['clean_tweet'])\n",
    "df_tfidf = tr.transform(df_train['clean_tweet'])\n",
    "pickle.dump(tr.vectorizer, open('../models/vectorizer.sav', 'wb'))\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(df_tfidf, df_train['label'],test_size=0.3,random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 8\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m f1_score\n\u001B[0;32m      7\u001B[0m model \u001B[38;5;241m=\u001B[39m LogisticRegression(random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,solver\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlbfgs\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(\u001B[43mx_train\u001B[49m, y_train)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = LogisticRegression(random_state=0,solver='lbfgs')\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.97822924, 0.02177076],\n       [0.97365865, 0.02634135],\n       [0.84763288, 0.15236712],\n       ...,\n       [0.95255636, 0.04744364],\n       [0.76914456, 0.23085544],\n       [0.94616521, 0.05383479]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_proba(x_valid)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m prediction_int \u001B[38;5;241m=\u001B[39m predictions[:,\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m\n\u001B[0;32m----> 2\u001B[0m prediction_int \u001B[38;5;241m=\u001B[39m prediction_int\u001B[38;5;241m.\u001B[39mastype(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mint\u001B[49m)\n\u001B[1;32m      3\u001B[0m log_tfidf \u001B[38;5;241m=\u001B[39m f1_score(y_valid, prediction_int)\n\u001B[1;32m      5\u001B[0m log_tfidf\n",
      "File \u001B[0;32m~/School/diploma_thesis/venv/lib/python3.10/site-packages/numpy/__init__.py:305\u001B[0m, in \u001B[0;36m__getattr__\u001B[0;34m(attr)\u001B[0m\n\u001B[1;32m    300\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    301\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn the future `np.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` will be defined as the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    302\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcorresponding NumPy scalar.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;129;01min\u001B[39;00m __former_attrs__:\n\u001B[0;32m--> 305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(__former_attrs__[attr])\n\u001B[1;32m    307\u001B[0m \u001B[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001B[39;00m\n\u001B[1;32m    308\u001B[0m \u001B[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001B[39;00m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001B[39;00m\n\u001B[1;32m    312\u001B[0m \u001B[38;5;66;03m# the full `numpy.testing` namespace\u001B[39;00m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtesting\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "prediction_int = predictions[:,1]>=0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "log_tfidf = f1_score(y_valid, prediction_int)\n",
    "\n",
    "log_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('../models/model.sav', 'wb'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.94886958, 0.05113042],\n       [0.93658547, 0.06341453],\n       [0.9530083 , 0.0469917 ]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = tr.transform(['This thing is pretty shit', 'I like this pretty much, it is super', 'ugly bad ugly'])\n",
    "model.predict_proba(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(random_state=1337)",
      "text/html": "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=1337)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=1337)</pre></div></div></div></div></div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import TomekLinks,RandomUnderSampler, CondensedNearestNeighbour,EditedNearestNeighbours\n",
    "#balancing\n",
    "\n",
    "tl = RandomUnderSampler(sampling_strategy='not minority',random_state=1337) # default - will remove the sample from the majority class\n",
    "#tl = TomekLinks(sampling_strategy='not minority',n_jobs = -1)\n",
    "x_train_res, y_train_res = tl.fit_resample(x_train, y_train)\n",
    "model2 = LogisticRegression(random_state=1337,solver='lbfgs')\n",
    "\n",
    "model2.fit(x_train_res, y_train_res)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "'LogisticRegression(random_state=1337)'"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pickle.dump(model, open('../models/model2.sav', 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "predictions = model2.predict_proba(x_valid)\n",
    "prediction_int = predictions[:,1]>=0.3\n",
    "prediction_int = prediction_int.astype(np.int32)\n",
    "log_tfidf = f1_score(y_valid, prediction_int)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.38324213, 0.61675787],\n       [0.6438945 , 0.3561055 ],\n       [0.57561075, 0.42438925],\n       [0.63779563, 0.36220437]])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = tr.transform([\"it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia \", 'This thing is pretty shit', 'I like this pretty much, it is super', 'ugly bad ugly'])\n",
    "model2.predict_proba(test_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def analyze_data(model, data_path, data_transformer, data_balancer=None, save_model=True, save_vectorizer=True, verbose=True):\n",
    "    data_name = data_path.split('/')[-1]\n",
    "    name = f\"{data_name}-{model.__repr__()}\"\n",
    "    if data_balancer is not None:\n",
    "        name += '-balanced'\n",
    "    if verbose:\n",
    "        print(f'Analyzing {name}', end='')\n",
    "    # Load data\n",
    "    df_train = pd.read_csv(data_path)\n",
    "    # Clean data\n",
    "    df_train['clean_tweet'] = np.vectorize(data_cleaner.clean_mentions)(df_train['tweet'])\n",
    "    df_train['clean_tweet'] = df_train['clean_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    df_train['clean_tweet'] = df_train['clean_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "    # Transform data\n",
    "    df_train['clean_tweet'] = data_transformer.stemming(df_train['clean_tweet'])\n",
    "    data_transformer.vectorizer_fit(df_train['clean_tweet'])\n",
    "    df_tfidf = data_transformer.transform(df_train['clean_tweet'])\n",
    "    if save_vectorizer:\n",
    "        pickle.dump(data_transformer.vectorizer, open(f'../models/{data_name}-{data_transformer.vectorizer.__repr__()}.sav', 'wb'))\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(df_tfidf, df_train['label'],test_size=0.3,random_state=1337)\n",
    "    # Balance data\n",
    "    if data_balancer is not None:\n",
    "        x_train, y_train = data_balancer.fit_resample(x_train, y_train)\n",
    "    # Fit model (HYPERPARAMS???)\n",
    "    model.fit(x_train, y_train)\n",
    "    if save_model:\n",
    "        pickle.dump(model, open(f'../models/{name}.sav', 'wb'))\n",
    "    # Predict\n",
    "    if isinstance(model, (LogisticRegression, GaussianNB)):\n",
    "        predictions = model.predict_proba(x_valid)\n",
    "        prediction_int = predictions[:,1]>=0.3\n",
    "        prediction_int = prediction_int.astype(np.int32)\n",
    "    else:\n",
    "        predictions = model.predict(x_valid)\n",
    "        prediction_int = predictions[:]>=0.3\n",
    "        prediction_int = prediction_int.astype(np.int32)\n",
    "    # Evaluate\n",
    "    metrics = {\n",
    "        'f1_score': f1_score(y_valid, prediction_int)\n",
    "    }\n",
    "    #\n",
    "    if verbose:\n",
    "        print(f' ->> Done')\n",
    "    return name, model, data_transformer ,predictions, metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing train.csv-LogisticRegression(random_state=0) ->> Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from data_transformation import TfidfDataTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import TomekLinks,RandomUnderSampler\n",
    "\n",
    "\n",
    "name, model, tr, pred, metrics = analyze_data(model=LogisticRegression(random_state=0,solver='lbfgs'), data_path='data/train.csv', data_transformer=TfidfDataTransformer(), data_balancer=RandomUnderSampler(sampling_strategy='not minority',random_state=1337))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing train.csv-LogisticRegression(random_state=0)-balanced ->> Done\n",
      "train.csv-LogisticRegression(random_state=0)-balanced\n",
      "[[0.43022813 0.56977187]\n",
      " [0.61919378 0.38080622]\n",
      " [0.95026837 0.04973163]\n",
      " [0.5947946  0.4052054 ]]\n",
      "f1: 0.21734216050379462\n",
      "Analyzing train.csv-GaussianNB()-balanced ->> Done\n",
      "train.csv-GaussianNB()-balanced\n",
      "[[1.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 1.00000000e+000]\n",
      " [1.18650412e-150 1.00000000e+000]\n",
      " [0.00000000e+000 1.00000000e+000]]\n",
      "f1: 0.2470515207945376\n",
      "Analyzing train.csv-DecisionTreeRegressor()-balanced"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DecisionTreeRegressor' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m test_data \u001B[38;5;241m=\u001B[39m tr\u001B[38;5;241m.\u001B[39mtransform([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mit\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms unbelievable that in the 21st century we\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124md need something like this. again. #neverump  #xenophobia \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThis thing is pretty shit\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlife\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mugly bad ugly\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m [LogisticRegression(random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,solver\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlbfgs\u001B[39m\u001B[38;5;124m'\u001B[39m), GaussianNB(), DecisionTreeRegressor()]:\n\u001B[1;32m----> 6\u001B[0m     name, model, tr, pred, metrics \u001B[38;5;241m=\u001B[39m \u001B[43manalyze_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata/train.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_transformer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mTfidfDataTransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_balancer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mRandomUnderSampler\u001B[49m\u001B[43m(\u001B[49m\u001B[43msampling_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnot minority\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1337\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28mprint\u001B[39m(name)\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28mprint\u001B[39m(model\u001B[38;5;241m.\u001B[39mpredict_proba(test_data))\n",
      "Cell \u001B[1;32mIn[23], line 29\u001B[0m, in \u001B[0;36manalyze_data\u001B[1;34m(model, data_path, data_transformer, data_balancer, save_model, save_vectorizer, verbose)\u001B[0m\n\u001B[0;32m     27\u001B[0m     pickle\u001B[38;5;241m.\u001B[39mdump(model, \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../models/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.sav\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# Predict\u001B[39;00m\n\u001B[1;32m---> 29\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_proba\u001B[49m(x_valid)\n\u001B[0;32m     30\u001B[0m prediction_int \u001B[38;5;241m=\u001B[39m predictions[:,\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m\n\u001B[0;32m     31\u001B[0m prediction_int \u001B[38;5;241m=\u001B[39m prediction_int\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mint32)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'DecisionTreeRegressor' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "test_data = tr.transform([\"it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia \", 'This thing is pretty shit', 'life', 'ugly bad ugly'])\n",
    "\n",
    "for m in [LogisticRegression(random_state=0,solver='lbfgs'), GaussianNB(), DecisionTreeRegressor()]:\n",
    "    name, model, tr, pred, metrics = analyze_data(model=m, data_path='data/train.csv', data_transformer=TfidfDataTransformer(), data_balancer=RandomUnderSampler(sampling_strategy='not minority',random_state=1337))\n",
    "    print(name)\n",
    "    print(model.predict_proba(test_data))\n",
    "    print(f'f1: {metrics[\"f1_score\"]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing train.csv-DecisionTreeRegressor()-balanced ->> Done\n",
      "train.csv-DecisionTreeRegressor()-balanced\n",
      "[1. 0. 0. 0.]\n",
      "f1: 0.3530196863530197\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "name, model, tr, pred, metrics = analyze_data(model=DecisionTreeRegressor(), data_path='data/train.csv', data_transformer=TfidfDataTransformer(), data_balancer=RandomUnderSampler(sampling_strategy='not minority',random_state=1337))\n",
    "print(name)\n",
    "print(model.predict(test_data))\n",
    "print(f'f1: {metrics[\"f1_score\"]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      1      2      3      4      5      6      7      8      9      \\\n",
      "31952    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "31953    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "31954    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "31955    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "       ...  31492  31493  31494  31495  31496  31497  31498  31499  31500  \\\n",
      "31952  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "31953  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "31954  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "31955  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "       31501  \n",
      "31952    0.0  \n",
      "31953    0.0  \n",
      "31954    0.0  \n",
      "31955    0.0  \n",
      "\n",
      "[4 rows x 31502 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 31502 features, but LogisticRegression is expecting 1000 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m test_data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mit\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms unbelievable that in the 21st century we\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124md need something like this. again. #neverump  #xenophobia \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThis thing is pretty shit\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlife\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mugly bad ugly\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      4\u001B[0m analyzer \u001B[38;5;241m=\u001B[39m Analyzer(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel.sav\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvectorizer.sav\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 5\u001B[0m \u001B[43manalyzer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_sentiment_analysis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mW:\\diploma_thesis\\src\\analyzer\\analyzer.py:38\u001B[0m, in \u001B[0;36mAnalyzer.get_sentiment_analysis\u001B[1;34m(self, texts)\u001B[0m\n\u001B[0;32m     36\u001B[0m data \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;241m-\u001B[39mdata_len:]\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28mprint\u001B[39m(data)\n\u001B[1;32m---> 38\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28mprint\u001B[39m(predictions)\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [x[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m predictions]\n",
      "File \u001B[1;32mW:\\diploma_thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1372\u001B[0m, in \u001B[0;36mLogisticRegression.predict_proba\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m   1364\u001B[0m ovr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmulti_class \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124movr\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarn\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   1365\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmulti_class \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1366\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1369\u001B[0m     )\n\u001B[0;32m   1370\u001B[0m )\n\u001B[0;32m   1371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ovr:\n\u001B[1;32m-> 1372\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_predict_proba_lr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1373\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1374\u001B[0m     decision \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecision_function(X)\n",
      "File \u001B[1;32mW:\\diploma_thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_base.py:434\u001B[0m, in \u001B[0;36mLinearClassifierMixin._predict_proba_lr\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    427\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_predict_proba_lr\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m    428\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Probability estimation for OvR logistic regression.\u001B[39;00m\n\u001B[0;32m    429\u001B[0m \n\u001B[0;32m    430\u001B[0m \u001B[38;5;124;03m    Positive class probabilities are computed as\u001B[39;00m\n\u001B[0;32m    431\u001B[0m \u001B[38;5;124;03m    1. / (1. + np.exp(-self.decision_function(X)));\u001B[39;00m\n\u001B[0;32m    432\u001B[0m \u001B[38;5;124;03m    multiclass is handled by normalizing that over all classes.\u001B[39;00m\n\u001B[0;32m    433\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 434\u001B[0m     prob \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecision_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    435\u001B[0m     expit(prob, out\u001B[38;5;241m=\u001B[39mprob)\n\u001B[0;32m    436\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m prob\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32mW:\\diploma_thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_base.py:400\u001B[0m, in \u001B[0;36mLinearClassifierMixin.decision_function\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    397\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    398\u001B[0m xp, _ \u001B[38;5;241m=\u001B[39m get_namespace(X)\n\u001B[1;32m--> 400\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    401\u001B[0m scores \u001B[38;5;241m=\u001B[39m safe_sparse_dot(X, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoef_\u001B[38;5;241m.\u001B[39mT, dense_output\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintercept_\n\u001B[0;32m    402\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m xp\u001B[38;5;241m.\u001B[39mreshape(scores, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m scores\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m scores\n",
      "File \u001B[1;32mW:\\diploma_thesis\\venv\\lib\\site-packages\\sklearn\\base.py:569\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    566\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m--> 569\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_n_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32mW:\\diploma_thesis\\venv\\lib\\site-packages\\sklearn\\base.py:370\u001B[0m, in \u001B[0;36mBaseEstimator._check_n_features\u001B[1;34m(self, X, reset)\u001B[0m\n\u001B[0;32m    367\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m    369\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_features \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_features_in_:\n\u001B[1;32m--> 370\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    371\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX has \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_features\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m features, but \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    372\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis expecting \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_features_in_\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m features as input.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    373\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: X has 31502 features, but LogisticRegression is expecting 1000 features as input."
     ]
    }
   ],
   "source": [
    "from analyzer.analyzer import Analyzer\n",
    "test_data = [\"it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia \", 'This thing is pretty shit', 'life', 'ugly bad ugly']\n",
    "\n",
    "analyzer = Analyzer('models', 'model.sav', 'vectorizer.sav')\n",
    "analyzer.get_sentiment_analysis(test_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c011ed6bf922ee54bc201eceecf54cf8e6293d9ca800dfe0b9a9fb97a052d906"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
